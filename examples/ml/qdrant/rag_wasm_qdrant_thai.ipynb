{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "be881a6d",
      "metadata": {
        "collapsed": true,
        "id": "be881a6d",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# Retrieval Augmented Generation (RAG) with Wasm and Qdrant\n",
        "\n",
        "In the ever-evolving landscape of AI, the consistency and reliability of Large Language Models (LLMs) remain a challenge. While these models can understand statistical relationships between words, they often fail to provide accurate factual responses. Because their internal knowledge may not be accurate, outputs can range from spot-on to nonsensical. Retrieval Augmented Generation (RAG) is a framework designed to bolster the accuracy of LLMs by grounding them in external knowledge bases. In this example, we'll demonstrate a streamlined  implementation of the RAG pipeline using only Qdrant and OpenAI SDKs. By harnessing Flag embedding's power, we can bypass additional frameworks' overhead.\n",
        "    \n",
        "This example assumes you understand the architecture necessary to carry out RAG. If this is new to you, please look at some introductory readings:\n",
        "* [Retrieval-Augmented Generation: To add knowledge](https://eugeneyan.com/writing/llm-patterns/#retrieval-augmented-generation-to-add-knowledge)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb044259",
      "metadata": {
        "id": "bb044259"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Let's start setting up all the pieces to implement the RAG pipeline. We will only use Qdrant and OpenAI SDKs, without any third-party libraries.\n",
        "\n",
        "### Preparing the environment\n",
        "\n",
        "We need just a few dependencies to implement the whole application, so let's start with installing the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "4ce9f81b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:32.977456Z",
          "start_time": "2023-09-27T10:06:30.203757Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4ce9f81b",
        "outputId": "29ed3ac4-8a22-41b3-e0f6-f247fc439b44",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: cohere in /Users/katopz/Library/Python/3.9/lib/python/site-packages (4.40)\n",
            "Requirement already satisfied: tiktoken in /Users/katopz/Library/Python/3.9/lib/python/site-packages (0.5.2)\n",
            "Requirement already satisfied: protobuf==3.20.3 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (3.20.3)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (4.5.0)\n",
            "Requirement already satisfied: qdrant-client in /Users/katopz/Library/Python/3.9/lib/python/site-packages (1.7.0)\n",
            "Requirement already satisfied: fastembed in /Users/katopz/Library/Python/3.9/lib/python/site-packages (0.1.3)\n",
            "Requirement already satisfied: openai in /Users/katopz/Library/Python/3.9/lib/python/site-packages (1.5.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from cohere) (3.9.1)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from cohere) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2.0,>=1.8 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from cohere) (1.9.2)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from cohere) (6.11.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from cohere) (1.26.18)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from qdrant-client) (1.60.0)\n",
            "Requirement already satisfied: grpcio-tools>=1.41.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from qdrant-client) (1.48.2)\n",
            "Requirement already satisfied: httpx>=0.14.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from httpx[http2]>=0.14.0->qdrant-client) (0.26.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from qdrant-client) (1.26.2)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from qdrant-client) (2.8.2)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from qdrant-client) (1.10.13)\n",
            "Requirement already satisfied: huggingface-hub==0.19.4 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from fastembed) (0.19.4)\n",
            "Requirement already satisfied: onnx<2.0,>=1.11 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from fastembed) (1.15.0)\n",
            "Requirement already satisfied: onnxruntime<2.0,>=1.15 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from fastembed) (1.16.3)\n",
            "Requirement already satisfied: tokenizers<0.16.0,>=0.15.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from fastembed) (0.15.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.65 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from fastembed) (4.66.1)\n",
            "Requirement already satisfied: filelock in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from huggingface-hub==0.19.4->fastembed) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from huggingface-hub==0.19.4->fastembed) (2023.12.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from huggingface-hub==0.19.4->fastembed) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from huggingface-hub==0.19.4->fastembed) (23.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from openai) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: sniffio in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0,>=3.0->cohere) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (58.0.4)\n",
            "Requirement already satisfied: certifi in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from httpx[http2]>=0.14.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: coloredlogs in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from onnxruntime<2.0,>=1.15->fastembed) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from onnxruntime<2.0,>=1.15->fastembed) (23.5.26)\n",
            "Requirement already satisfied: sympy in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from onnxruntime<2.0,>=1.15->fastembed) (1.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client) (4.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from coloredlogs->onnxruntime<2.0,>=1.15->fastembed) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/katopz/Library/Python/3.9/lib/python/site-packages (from sympy->onnxruntime<2.0,>=1.15->fastembed) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# To run in Google Colab, need to install these libraries, then restart session\n",
        "!pip install cohere tiktoken protobuf==3.20.3 typing-extensions==4.5.0 qdrant-client fastembed openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aae4382a",
      "metadata": {
        "id": "aae4382a"
      },
      "source": [
        "[Qdrant](https://qdrant.tech) will act as a knowledge base providing the context information for the prompts we'll be sending to the LLM. There are various ways of running Qdrant, but we'll simply use the Docker container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "e8f4456c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:34.283299Z",
          "start_time": "2023-09-27T10:06:32.980517Z"
        },
        "id": "e8f4456c"
      },
      "outputs": [],
      "source": [
        "# Docker\n",
        "# !docker run -p \"6333:6333\" -p \"6334:6334\" --name \"rag-openai-qdrant\" --rm -d qdrant/qdrant:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a74c7a21",
      "metadata": {
        "id": "a74c7a21"
      },
      "source": [
        "### Creating the collection\n",
        "\n",
        "Qdrant [collection](https://qdrant.tech/documentation/concepts/collections/) is the basic unit of organizing your data. Each collection is a named set of points (vectors with a payload) among which you can search. After connecting to our running Qdrant container, we can check whether we already have some collections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd8966b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:36.242783Z",
          "start_time": "2023-09-27T10:06:34.289290Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dd8966b",
        "outputId": "c67930a8-7c70-478a-b155-6823ebd955b0"
      },
      "outputs": [],
      "source": [
        "import qdrant_client\n",
        "\n",
        "# Memory\n",
        "client = qdrant_client.QdrantClient(\":memory:\")\n",
        "client.get_collections()\n",
        "client.set_model(\"intfloat/multilingual-e5-large\")\n",
        "client.recreate_collection(\n",
        "    collection_name=\"knowledge-base\",\n",
        "    vectors_config=client.get_fastembed_vector_params(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f54205",
      "metadata": {
        "id": "23f54205"
      },
      "source": [
        "### Building the knowledge base\n",
        "\n",
        "Qdrant will use vector embeddings of our facts to enrich the original prompt with some context. Thus, we need to store the vector embeddings and the texts used to generate them. All our facts will have a JSON payload with a single attribute and look as follows:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"document\": \"Binary Quantization is a method of reducing the memory usage even up to 40 times!\"\n",
        "}\n",
        "```\n",
        "\n",
        "This structure is required by [FastEmbed](https://qdrant.github.io/fastembed/), a library that simplifies managing the vectors, as you don't have to calculate them on your own. It's also possible to use an existing collection, However, all the code snippets will assume this data structure. Adjust your examples to work with a different schema.\n",
        "\n",
        "FastEmbed will automatically create the collection if it doesn't exist. Knowing that we are set to add our documents to a collection, which we'll call `knowledge-base`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "af548c07",
      "metadata": {},
      "outputs": [],
      "source": [
        "documents = [\n",
        "        \"ต๊อบ ชอบกินมะพร้าว\",\n",
        "        \"ต๊อบ ชอบกินถั่ว\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "43154775",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:36.692231Z",
          "start_time": "2023-09-27T10:06:36.245915Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43154775",
        "outputId": "4e99e6b8-11fd-4271-ed4d-c26cafcfc294"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['de66bb58888e4f7db61e8561e88d5f89', '920df6cbc1c0466ca9cfdb816a3fd51d']"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# metadata = [{\"text\": item} for item in documents]\n",
        "\n",
        "client.add(\n",
        "    collection_name=\"knowledge-base\",\n",
        "    documents=documents,\n",
        "    # metadata=metadata,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b36bddd6",
      "metadata": {
        "id": "b36bddd6"
      },
      "source": [
        "## Retrieval Augmented Generation\n",
        "\n",
        "RAG changes the way we interact with Large Language Models. We're converting a knowledge-oriented task, in which the model may create a counterfactual answer, into a language-oriented task. The latter expects the model to extract meaningful information and generate an answer. LLMs, when implemented correctly, are supposed to be carrying out language-oriented tasks.\n",
        "\n",
        "The task starts with the original prompt sent by the user. The same prompt is then vectorized and used as a search query for the most relevant facts. Those facts are combined with the original prompt to build a longer prompt containing more information.\n",
        "\n",
        "But let's start simply by asking our question directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "ed31ca63",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:36.695165Z",
          "start_time": "2023-09-27T10:06:36.695150Z"
        },
        "id": "ed31ca63"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "ต๊อบชอบกินอะไร?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d2d7dd",
      "metadata": {
        "id": "e7d2d7dd"
      },
      "source": [
        "We will use `TGI` via dokcer + WSL2 + RTX4090"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "30e8669e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:36.696985Z",
          "start_time": "2023-09-27T10:06:36.696959Z"
        },
        "id": "30e8669e"
      },
      "outputs": [],
      "source": [
        "# !model=SeaLLMs/SeaLLM-7B-Chat\n",
        "# !volume=$PWD/data\n",
        "\n",
        "# !docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.3 --model-id $model "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87948f58",
      "metadata": {},
      "source": [
        "Or `WasmEdge` via https://wasmedge.org/docs/start/overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "19d5cb4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !curl -LO https://huggingface.co/parinzee/SeaLLM-7B-Chat-GGUF/resolve/main/seallm-7b-chat.q4_k_m.gguf\n",
        "# !curl -LO https://code.flows.network/webhook/iwYN1SdN3AmPgR5ao5Gt/llama-api-server.wasm\n",
        "# !curl -LO https://code.flows.network/webhook/iwYN1SdN3AmPgR5ao5Gt/llama-chat.wasm\n",
        "# !curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- --plugins wasmedge_rustls wasi_nn-ggml\n",
        "# !source $HOME/.wasmedge/env\n",
        "# !wasmedge --dir .:. --nn-preload default:GGML:AUTO:seallm-7b-chat.q4_k_m.gguf llama-api-server.wasm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "8e059517",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"id\":\"9bc7c7aa-598e-4d39-836b-ffa161fbd2c3\",\"object\":\"chat.completion\",\"created\":1704545558,\"model\":\"openthaigpt-1.0.0-beta-13b-chat\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"กรุงเทพฯ\"},\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":35,\"completion_tokens\":2,\"total_tokens\":37}}"
          ]
        }
      ],
      "source": [
        "!curl -X POST http://0.0.0.0:8080/v1/chat/completions -H 'accept:application/json' -H 'Content-Type: application/json' -d '{\"messages\":[{\"role\":\"system\", \"content\":\"You are a helpful AI assistant\"}, {\"role\":\"user\", \"content\":\"กทม ย่อมาจากอะไร\"}], \"model\":\"openthaigpt-1.0.0-beta-13b-chat\"}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "c73a09e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# curl -X POST http://0.0.0.0:8080/v1/chat/completions -H 'accept:application/json' -H 'Content-Type: application/json' -d '{\"messages\":[{\"role\":\"system\", \"content\":\"You are a helpful AI assistant\"}, {\"role\":\"user\", \"content\":\"กทม ย่อมาจากอะไร\"}], \"model\":\"openthaigpt-1.0.0-beta-13b-chat\"}'\n",
        "\n",
        "# !curl 127.0.0.1:8080/generate \\\n",
        "#     -X POST \\\n",
        "#     -d '{\"inputs\":\"แอดต๊อบชอบกินอะไร?\",\"parameters\":{\"max_new_tokens\":20}}' \\\n",
        "#     -H 'Content-Type: application/json'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b420d81d",
      "metadata": {
        "id": "b420d81d"
      },
      "source": [
        "### Extending the prompt\n",
        "\n",
        "Even though the original answer sounds credible, it didn't answer our question correctly. Instead, it gave us a generic description of an application stack. To improve the results, enriching the original prompt with the descriptions of the tools available seems like one of the possibilities. Let's use a semantic knowledge base to augment the prompt with the descriptions of different technologies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "ce791ba3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:36.702641Z",
          "start_time": "2023-09-27T10:06:36.702619Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce791ba3",
        "outputId": "02369049-fe1e-4f1a-d548-7b17c33b990e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[QueryResponse(id='de66bb58888e4f7db61e8561e88d5f89', embedding=None, metadata={'document': 'ต๊อบ ชอบกินมะพร้าว'}, document='ต๊อบ ชอบกินมะพร้าว', score=0.9020121864698284),\n",
              " QueryResponse(id='920df6cbc1c0466ca9cfdb816a3fd51d', embedding=None, metadata={'document': 'ต๊อบ ชอบกินถั่ว'}, document='ต๊อบ ชอบกินถั่ว', score=0.8977321732302992)]"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = client.query(\n",
        "    collection_name=\"knowledge-base\",\n",
        "    query_text=prompt,\n",
        "    limit=10,\n",
        ")\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "f2LSyAcKK9V0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2LSyAcKK9V0",
        "outputId": "085f6f95-d6ab-46ef-850c-b329fc12399e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ต๊อบ ชอบกินมะพร้าว', 'ต๊อบ ชอบกินถั่ว']"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[result.metadata['document'] for result in results]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6640067",
      "metadata": {
        "id": "c6640067"
      },
      "source": [
        "We used the original prompt to perform a semantic search over the set of tool descriptions. Now we can use these descriptions to augment the prompt and create more context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "a16d8549",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a16d8549",
        "outputId": "6b538b86-fa68-4768-ae96-7516f7d68355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ต๊อบ ชอบกินมะพร้าว\n",
            "ต๊อบ ชอบกินถั่ว\n"
          ]
        }
      ],
      "source": [
        "context = \"\\n\".join(r.document for r in results)\n",
        "print(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2c04a4e",
      "metadata": {
        "id": "a2c04a4e"
      },
      "source": [
        "Finally, let's build a metaprompt, the combination of the assumed role of the LLM, the original question, and the results from our semantic search that will force our LLM to use the provided context.\n",
        "\n",
        "By doing this, we effectively convert the knowledge-oriented task into a language task and hopefully reduce the chances of hallucinations. It also should make the response sound more relevant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "1fc9a98b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fc9a98b",
        "outputId": "d60fbd68-fbc2-451f-e179-c38b8d771fac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You will reply in Thai.\n",
            "Answer the following question using the provided context.\n",
            "If you can't find the answer, do not pretend you know it, but answer \"ไม่รู้\".\n",
            "\n",
            "Question: ต๊อบชอบกินอะไร?\n",
            "\n",
            "Context:\n",
            "ต๊อบ ชอบกินมะพร้าว\n",
            "ต๊อบ ชอบกินถั่ว\n",
            "\n",
            "Answer:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "metaprompt = f\"\"\"\n",
        "You will reply in Thai.\n",
        "Answer the following question using the provided context.\n",
        "If you can't find the answer, do not pretend you know it, but answer \"ไม่รู้\".\n",
        "\n",
        "Question: {prompt.strip()}\n",
        "\n",
        "Context:\n",
        "{context.strip()}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Look at the full metaprompt\n",
        "print(metaprompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f1a7678",
      "metadata": {
        "id": "9f1a7678"
      },
      "source": [
        "Our current prompt is much longer, and we also used a couple of strategies to make the responses even better:\n",
        "\n",
        "1. The LLM has the role of software architect.\n",
        "2. We provide more context to answer the question.\n",
        "3. If the context contains no meaningful information, the model shouldn't make up an answer.\n",
        "\n",
        "Let's find out if that works as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "d6beaee5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_completion(context, prompt):\n",
        "    metaprompt = f\"\"\"You will reply in Thai.\n",
        "Answer the following question using the provided context.\n",
        "If you can't find the answer, do not pretend you know it, but answer \"ไม่รู้\".\n",
        "If it out of context below, do not pretend you know it, but answer \"ไม่รู้ๆๆ\".\n",
        "\n",
        "Context:\n",
        "{context.strip()}\"\"\"\n",
        "    \n",
        "    url = 'http://0.0.0.0:8080/v1/chat/completions'\n",
        "    headers = {\n",
        "        'accept': 'application/json',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    print(metaprompt)\n",
        "\n",
        "    data = {\n",
        "        'messages': [\n",
        "            {'role': 'system', 'content': metaprompt},\n",
        "            {'role': 'user', 'content': prompt.strip()}\n",
        "        ],\n",
        "        'model': 'openthaigpt-1.0.0-beta-13b-chat'\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "    completion_json = response.json()\n",
        "    completion = completion_json['choices'][0]['message']['content']\n",
        "\n",
        "    return completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "b33d08df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You will reply in Thai.\n",
            "Answer the following question using the provided context.\n",
            "If you can't find the answer, do not pretend you know it, but answer \"ไม่รู้\".\n",
            "If it out of context below, do not pretend you know it, but answer \"ไม่รู้ๆๆ\".\n",
            "\n",
            "Context:\n",
            "ต๊อบ ชอบกินมะพร้าว\n",
            "ต๊อบ ชอบกินถั่ว\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'ไม่รู้'"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_completion(context, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4120e1-9899-4caa-b974-51d9b3a485be",
      "metadata": {
        "id": "1c4120e1-9899-4caa-b974-51d9b3a485be"
      },
      "source": [
        "### Testing out the RAG pipeline\n",
        "\n",
        "By leveraging the semantic context we provided our model is doing a better job answering the question. Let's enclose the RAG as a function, so we can call it more easily for different prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "62ed09d1-2c90-4ffc-9f1d-7beb87bab78b",
      "metadata": {
        "id": "62ed09d1-2c90-4ffc-9f1d-7beb87bab78b"
      },
      "outputs": [],
      "source": [
        "async def rag(question: str, n_points: int = 10) -> str:\n",
        "    results = client.query(\n",
        "        collection_name=\"knowledge-base\",\n",
        "        query_text=question,\n",
        "        limit=n_points,\n",
        "    )\n",
        "\n",
        "    context = \"\\n\".join(r.document for r in results)\n",
        "\n",
        "    return  get_completion(context, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86fecd76-9a0b-4ad1-9097-b1d292a618ac",
      "metadata": {
        "id": "86fecd76-9a0b-4ad1-9097-b1d292a618ac"
      },
      "source": [
        "Now it's easier to ask a broad range of questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "aa0fdead-a115-4fcd-88dc-5cc718dc0544",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "aa0fdead-a115-4fcd-88dc-5cc718dc0544",
        "outputId": "f20fc1d7-d918-4fe4-8c28-56abe2057aa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You will reply in Thai.\n",
            "Answer the following question using the provided context.\n",
            "If you can't find the answer, do not pretend you know it, but answer \"ไม่รู้\".\n",
            "If it out of context below, do not pretend you know it, but answer \"ไม่รู้ๆๆ\".\n",
            "\n",
            "Context:\n",
            "ต๊อบ ชอบกินมะพร้าว\n",
            "ต๊อบ ชอบกินถั่ว\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'ต็อบชอบกินมะพร้าวและถั่ว'"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await rag(\"ต๊อบ ชอบกินอะไร?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "7324c127-c140-410a-ab19-87a5babce023",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7324c127-c140-410a-ab19-87a5babce023",
        "outputId": "ab714dfe-c98d-4389-ca23-b9f4421c372a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You will reply in Thai.\n",
            "Answer the following question using the provided context.\n",
            "If you can't find the answer, do not pretend you know it, but answer \"ไม่รู้\".\n",
            "If it out of context below, do not pretend you know it, but answer \"ไม่รู้ๆๆ\".\n",
            "\n",
            "Context:\n",
            "ต๊อบ ชอบกินมะพร้าว\n",
            "ต๊อบ ชอบกินถั่ว\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'ไม่รู้'"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await rag(\"หญิง ชอบกินอะไร?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fe56730-ed41-42c1-9c33-de3849c60b65",
      "metadata": {
        "id": "6fe56730-ed41-42c1-9c33-de3849c60b65"
      },
      "source": [
        "Our model can now:\n",
        "\n",
        "1. Take advantage of the knowledge in our vector datastore.\n",
        "2. Answer, based on the provided context, that it can not provide an answer.\n",
        "\n",
        "We have just shown a useful mechanism to mitigate the risks of hallucinations in Large Language Models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "968a3ae6",
      "metadata": {
        "id": "968a3ae6"
      },
      "source": [
        "### Cleaning up the environment\n",
        "\n",
        "If you wish to continue playing with the RAG application we created, don't do the code below. However, it's always good to clean up the environment, so nothing is left dangling. We'll show you how to remove the Qdrant container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "a0729043",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T10:06:36.704761Z",
          "start_time": "2023-09-27T10:06:36.704742Z"
        },
        "id": "a0729043",
        "outputId": "b5275dd3-628d-42d8-a1ac-a9dbf13511ed"
      },
      "outputs": [],
      "source": [
        "# !docker kill rag-openai-qdrant\n",
        "# !docker rm rag-openai-qdrant"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
