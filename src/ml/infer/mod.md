# Inferences

- [x] [candle](./wasmedge.md) // MacOS/Windows/WSL2 // 27GB // 7.71 tokens/sec
- [x] [llamafile](https://future.mozilla.org/blog/introducing-llamafile/) // MacOS/Windows // 28GB (no gc) // 34.68 tokens/sec
- [x] [WasmEdge](./wasmedge.md) // MacOS/Windows/WSL2
- [x] [TabbyML](./tabbyml.md) // MacOS/Windows/WSL2
- [x] [llama.cpp](https://github.com/ggerganov/llama.cpp) // MacOS/Windows // 13GB // 34.01 tokens/sec
- [x] [ollama](./ollama.md) // MacOS/Windows/WSL2
