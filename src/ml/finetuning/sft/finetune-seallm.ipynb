{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f76265",
   "metadata": {
    "papermill": {
     "duration": 0.01382,
     "end_time": "2023-12-14T06:20:59.424944",
     "exception": false,
     "start_time": "2023-12-14T06:20:59.411124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fine-tuning LLM on Your Own Dataset with QLoRA on a Single GPU\n",
    "\n",
    "Fine-tune the LLM base model on a custom dataset. We'll use the QLoRa technique to train an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd3a7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv .venv\n",
    "!source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3be3533",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:20:59.452704Z",
     "iopub.status.busy": "2023-12-14T06:20:59.452131Z",
     "iopub.status.idle": "2023-12-14T06:21:00.439239Z",
     "shell.execute_reply": "2023-12-14T06:21:00.438154Z"
    },
    "papermill": {
     "duration": 1.003781,
     "end_time": "2023-12-14T06:21:00.441780",
     "exception": false,
     "start_time": "2023-12-14T06:20:59.437999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar  9 22:56:59 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 546.01       CUDA Version: 12.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  Off |\n",
      "|  0%   41C    P8    22W / 450W |  16586MiB / 24564MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4608      C   /python3.10                     N/A      |\n",
      "|    0   N/A  N/A      7836      C   /wasmedge                       N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe9ca3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq fastembed==\"0.2.1\" --progress-bar off\n",
    "!pip install -qqq tokenizers==\"0.15.2\" --progress-bar off\n",
    "!pip install -qqq loguru==\"0.7.2\" --progress-bar off\n",
    "!pip install -qqq tqdm==\"4.66.0\" --progress-bar off\n",
    "!pip install -qqq scikit-learn --progress-bar off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7cf4baf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:21:00.469842Z",
     "iopub.status.busy": "2023-12-14T06:21:00.469529Z",
     "iopub.status.idle": "2023-12-14T06:22:11.140453Z",
     "shell.execute_reply": "2023-12-14T06:22:11.139188Z"
    },
    "papermill": {
     "duration": 70.687848,
     "end_time": "2023-12-14T06:22:11.143164",
     "exception": false,
     "start_time": "2023-12-14T06:21:00.455316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install ipywidgets\n",
    "!pip install -qqq torch --progress-bar off\n",
    "!pip install -qqq transformers==4.37.0 --progress-bar off\n",
    "!pip install -qqq datasets==2.14.4 --progress-bar off\n",
    "!pip install -qqq peft==0.8.2 --progress-bar off\n",
    "!pip install -qqq bitsandbytes==0.42.0 --progress-bar off\n",
    "!pip install -qqq trl==0.7.4 --progress-bar off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23e679",
   "metadata": {
    "papermill": {
     "duration": 0.015811,
     "end_time": "2023-12-14T06:22:11.173141",
     "exception": false,
     "start_time": "2023-12-14T06:22:11.157330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aba51f09",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2023-12-14T06:22:11.187108",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katopz/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from trl import SFTTrainer\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    AutoPeftModelForCausalLM\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3ff2e0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:06:29.578866Z",
     "iopub.status.busy": "2023-12-14T04:06:29.578356Z",
     "iopub.status.idle": "2023-12-14T04:06:29.584058Z",
     "shell.execute_reply": "2023-12-14T04:06:29.583205Z",
     "shell.execute_reply.started": "2023-12-14T04:06:29.578840Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"using {DEVICE} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "268f3685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:06:29.586502Z",
     "iopub.status.busy": "2023-12-14T04:06:29.586097Z",
     "iopub.status.idle": "2023-12-14T04:06:29.600711Z",
     "shell.execute_reply": "2023-12-14T04:06:29.600031Z",
     "shell.execute_reply.started": "2023-12-14T04:06:29.586463Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training prompt for instruction finetuning using '###' format\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\" คุณเป็นนักเรียนที่คอยตอบคำถาม โดยข้างล่างต่อไปนี้คือคำถาม จงตอบคำถามต่อไปนี้ \"\"\"\n",
    "\n",
    "\n",
    "###------------------------------ Process Dataset ------------------------------###\n",
    "def generate_training_prompt(\n",
    "    question: str, answer: str,\n",
    "    system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"### คำสั่ง:\n",
    "{system_prompt.strip()}\n",
    "\n",
    "### คำถาม:\n",
    "{question.strip()}\n",
    "\n",
    "### คำตอบ:\n",
    "{answer.strip()}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def process_dataset(data: pd.DataFrame):\n",
    "    data[\"text\"] = data.apply(\n",
    "        lambda row: generate_training_prompt(\n",
    "            row[\"question\"], row[\"answer\"]\n",
    "        ), axis=1\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a18d0de2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:06:29.601814Z",
     "iopub.status.busy": "2023-12-14T04:06:29.601553Z",
     "iopub.status.idle": "2023-12-14T04:06:29.635672Z",
     "shell.execute_reply": "2023-12-14T04:06:29.634644Z",
     "shell.execute_reply.started": "2023-12-14T04:06:29.601791Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/katopz/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token=\"YOUR_HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af658489",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Loading the model\n",
    "\n",
    "Define the [Lora Config](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.LoraConfig) with:\n",
    "\n",
    "- `task_type`, token classification **(TaskType.TOKEN_CLS)**\n",
    "- `r`, the dimension of the low-rank matrices\n",
    "- `lora_alpha`, scaling factor for the weight matrices\n",
    "- `lora_dropout`, dropout probability of the LoRA layers\n",
    "- `bias`, set to **all** to train all bias parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "941862c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:06:40.822300Z",
     "iopub.status.busy": "2023-12-14T04:06:40.821908Z",
     "iopub.status.idle": "2023-12-14T04:06:40.830192Z",
     "shell.execute_reply": "2023-12-14T04:06:40.829209Z",
     "shell.execute_reply.started": "2023-12-14T04:06:40.822271Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"SeaLLMs/SeaLLM-7B-v2\"  # @param [\"pythainlp/wangchanglm-7.5B-sft-enth-sharded\", \"TinyPixel/Llama-2-7B-bf16-sharded\", \"SeaLLMs/SeaLLM-7B-Chat\"]\n",
    "\n",
    "# Load both LLM model and tokenizer\n",
    "def load_LLM_and_tokenizer():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens = False\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1032cf47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:06:43.951397Z",
     "iopub.status.busy": "2023-12-14T04:06:43.950763Z",
     "iopub.status.idle": "2023-12-14T04:10:13.930476Z",
     "shell.execute_reply": "2023-12-14T04:10:13.929386Z",
     "shell.execute_reply.started": "2023-12-14T04:06:43.951366Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, tokenizer = load_LLM_and_tokenizer()\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b1fbdb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ea09619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:19:59.007736Z",
     "iopub.status.busy": "2023-12-14T04:19:59.007346Z",
     "iopub.status.idle": "2023-12-14T04:19:59.069678Z",
     "shell.execute_reply": "2023-12-14T04:19:59.068734Z",
     "shell.execute_reply.started": "2023-12-14T04:19:59.007695Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sysid</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>640696</td>\n",
       "      <td>ชื่ออะไร</td>\n",
       "      <td>ชื่อต๊อบ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>640696</td>\n",
       "      <td>ต๊อบชอบกินอะไร</td>\n",
       "      <td>ถั่ว</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>640696</td>\n",
       "      <td>ต๊อบชอบไปไหน</td>\n",
       "      <td>ไปเที่ยว</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>320422</td>\n",
       "      <td>ต๊อบชอบสีอะไร</td>\n",
       "      <td>เทา</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>320422</td>\n",
       "      <td>ต๊อบสูงเท่าไหร่</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sysid         question    answer\n",
       "0  640696         ชื่ออะไร  ชื่อต๊อบ\n",
       "1  640696   ต๊อบชอบกินอะไร      ถั่ว\n",
       "2  640696     ต๊อบชอบไปไหน  ไปเที่ยว\n",
       "3  320422    ต๊อบชอบสีอะไร       เทา\n",
       "4  320422  ต๊อบสูงเท่าไหร่       173"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "law_dataset = pd.read_csv(\"./test.csv\")\n",
    "law_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7421502b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:20:01.613820Z",
     "iopub.status.busy": "2023-12-14T04:20:01.613415Z",
     "iopub.status.idle": "2023-12-14T04:20:01.620163Z",
     "shell.execute_reply": "2023-12-14T04:20:01.619144Z",
     "shell.execute_reply.started": "2023-12-14T04:20:01.613787Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "law_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a04dfbe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:20:03.193936Z",
     "iopub.status.busy": "2023-12-14T04:20:03.193288Z",
     "iopub.status.idle": "2023-12-14T04:20:03.200126Z",
     "shell.execute_reply": "2023-12-14T04:20:03.199251Z",
     "shell.execute_reply.started": "2023-12-14T04:20:03.193903Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: limit first 1000 rows for lower resource\n",
    "law_dataset = law_dataset[:100]\n",
    "law_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2d0c74c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:20:04.976844Z",
     "iopub.status.busy": "2023-12-14T04:20:04.976473Z",
     "iopub.status.idle": "2023-12-14T04:20:04.987645Z",
     "shell.execute_reply": "2023-12-14T04:20:04.986735Z",
     "shell.execute_reply.started": "2023-12-14T04:20:04.976813Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (79, 3)\n",
      "Validation set shape: (10, 3)\n",
      "Test set shape: (10, 3)\n"
     ]
    }
   ],
   "source": [
    "# Specify the proportions for train, validation, and test sets\n",
    "train_ratio = 0.8\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Split the dataset\n",
    "train_data, temp_data = train_test_split(law_dataset, test_size=1 - train_ratio, random_state=42)\n",
    "validation_data, test_data = train_test_split(temp_data, test_size=test_ratio/(test_ratio + validation_ratio), random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting DataFrames\n",
    "print(\"Train set shape:\", train_data.shape)\n",
    "print(\"Validation set shape:\", validation_data.shape)\n",
    "print(\"Test set shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "926ac7b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:20:07.054983Z",
     "iopub.status.busy": "2023-12-14T04:20:07.054593Z",
     "iopub.status.idle": "2023-12-14T04:20:07.077465Z",
     "shell.execute_reply": "2023-12-14T04:20:07.076543Z",
     "shell.execute_reply.started": "2023-12-14T04:20:07.054952Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = process_dataset(train_data)\n",
    "validation_data = process_dataset(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59337dd1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "validation_dataset = Dataset.from_pandas(validation_data)\n",
    "\n",
    "combine_dataset = DatasetDict()\n",
    "combine_dataset['train'] = train_dataset\n",
    "combine_dataset['validation'] = validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17cbe176",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:20:10.549875Z",
     "iopub.status.busy": "2023-12-14T04:20:10.548994Z",
     "iopub.status.idle": "2023-12-14T04:20:10.555605Z",
     "shell.execute_reply": "2023-12-14T04:20:10.554623Z",
     "shell.execute_reply.started": "2023-12-14T04:20:10.549842Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sysid', 'question', 'answer', 'text', '__index_level_0__'],\n",
       "        num_rows: 79\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sysid', 'question', 'answer', 'text', '__index_level_0__'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c0d30",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Inference with Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7295a865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ต๊อบชอบไปไหน</td>\n",
       "      <td>ไปเที่ยว</td>\n",
       "      <td>### คำสั่ง:\\nข้างล่างต่อไปนี้คือคำถาม จงตอบคำถ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ต๊อบชอบไปไหน</td>\n",
       "      <td>ไปเที่ยว</td>\n",
       "      <td>### คำสั่ง:\\nข้างล่างต่อไปนี้คือคำถาม จงตอบคำถ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ต๊อบสูงเท่าไหร่</td>\n",
       "      <td>173</td>\n",
       "      <td>### คำสั่ง:\\nข้างล่างต่อไปนี้คือคำถาม จงตอบคำถ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ชื่ออะไร</td>\n",
       "      <td>ชื่อต๊อบ</td>\n",
       "      <td>### คำสั่ง:\\nข้างล่างต่อไปนี้คือคำถาม จงตอบคำถ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ชื่ออะไร</td>\n",
       "      <td>ชื่อต๊อบ</td>\n",
       "      <td>### คำสั่ง:\\nข้างล่างต่อไปนี้คือคำถาม จงตอบคำถ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          question  response  \\\n",
       "0     ต๊อบชอบไปไหน  ไปเที่ยว   \n",
       "1     ต๊อบชอบไปไหน  ไปเที่ยว   \n",
       "2  ต๊อบสูงเท่าไหร่       173   \n",
       "3         ชื่ออะไร  ชื่อต๊อบ   \n",
       "4         ชื่ออะไร  ชื่อต๊อบ   \n",
       "\n",
       "                                                text  \n",
       "0  ### คำสั่ง:\\nข้างล่างต่อไปนี้คือคำถาม จงตอบคำถ...  \n",
       "1  ### คำสั่ง:\\nข้างล่างต่อไปนี้คือคำถาม จงตอบคำถ...  \n",
       "2  ### คำสั่ง:\\nข้างล่างต่อไปนี้คือคำถาม จงตอบคำถ...  \n",
       "3  ### คำสั่ง:\\nข้างล่างต่อไปนี้คือคำถาม จงตอบคำถ...  \n",
       "4  ### คำสั่ง:\\nข้างล่างต่อไปนี้คือคำถาม จงตอบคำถ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INFERENCE_SYSTEM_PROMPT = \"\"\" ข้างล่างต่อไปนี้คือคำถาม จงตอบคำถามต่อไปนี้ \"\"\"\n",
    "\n",
    "max_new_tokens = 256    # @param {type: \"integer\"}\n",
    "temperature = 0.0001    # @param {type: \"number\"}\n",
    "\n",
    "def generate_answer(model, text: str):\n",
    "    batch = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(\n",
    "            input_ids=batch[\"input_ids\"].to(DEVICE),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            no_repeat_ngram_size=2,\n",
    "            typical_p=1.,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output_tokens[0][len(batch[\"input_ids\"][0]):], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def generate_inference_prompt(\n",
    "    question: str,\n",
    "    system_prompt: str = INFERENCE_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"### คำสั่ง:\n",
    "{system_prompt.strip()}\n",
    "\n",
    "### คำถาม:\n",
    "{question.strip()}\n",
    "\n",
    "### คำตอบ:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "examples = []\n",
    "for index, data_point in test_data.head(5).iterrows():\n",
    "    question = data_point[\"question\"]\n",
    "    label = data_point[\"answer\"]\n",
    "    examples.append({\n",
    "        \"question\": question,\n",
    "        \"response\": label,\n",
    "        \"text\": generate_inference_prompt(question)\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame(examples)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6710a3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "643a6ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:20:15.311619Z",
     "iopub.status.busy": "2023-12-14T04:20:15.311232Z",
     "iopub.status.idle": "2023-12-14T04:20:15.317328Z",
     "shell.execute_reply": "2023-12-14T04:20:15.316235Z",
     "shell.execute_reply.started": "2023-12-14T04:20:15.311571Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ชื่ออะไร\n"
     ]
    }
   ],
   "source": [
    "example = test_df.iloc[3]\n",
    "print(example.question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57d27263",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 141 ms, sys: 152 ms, total: 293 ms\n",
      "Wall time: 293 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = generate_answer(model, example.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f13a048b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T09:16:28.363573Z",
     "iopub.status.busy": "2023-12-13T09:16:28.362848Z",
     "iopub.status.idle": "2023-12-13T09:16:28.368423Z",
     "shell.execute_reply": "2023-12-13T09:16:28.367406Z",
     "shell.execute_reply.started": "2023-12-13T09:16:28.363543Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a34025f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T09:16:28.369980Z",
     "iopub.status.busy": "2023-12-13T09:16:28.369648Z",
     "iopub.status.idle": "2023-12-13T09:16:28.382742Z",
     "shell.execute_reply": "2023-12-13T09:16:28.381822Z",
     "shell.execute_reply.started": "2023-12-13T09:16:28.369941Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ชื่อต๊อบ'\n"
     ]
    }
   ],
   "source": [
    "pprint(example.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f2339",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Finetune QLoRA\n",
    "\n",
    "Here we will use the `SFTTrainer` from [TRL library](https://huggingface.co/docs/trl/main/en/sft_trainer) that gives a wrapper around transformers `Trainer` to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "352773e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T09:17:11.591227Z",
     "iopub.status.busy": "2023-12-13T09:17:11.590836Z",
     "iopub.status.idle": "2023-12-13T09:17:11.601958Z",
     "shell.execute_reply": "2023-12-13T09:17:11.600830Z",
     "shell.execute_reply.started": "2023-12-13T09:17:11.591197Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(48384, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=48384, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42ac5494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T09:17:13.788741Z",
     "iopub.status.busy": "2023-12-13T09:17:13.787913Z",
     "iopub.status.idle": "2023-12-13T09:17:13.795308Z",
     "shell.execute_reply": "2023-12-13T09:17:13.794264Z",
     "shell.execute_reply.started": "2023-12-13T09:17:13.788709Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n",
       " 'load_in_8bit': False,\n",
       " 'load_in_4bit': True,\n",
       " 'llm_int8_threshold': 6.0,\n",
       " 'llm_int8_skip_modules': None,\n",
       " 'llm_int8_enable_fp32_cpu_offload': False,\n",
       " 'llm_int8_has_fp16_weight': False,\n",
       " 'bnb_4bit_quant_type': 'nf4',\n",
       " 'bnb_4bit_use_double_quant': False,\n",
       " 'bnb_4bit_compute_dtype': 'float16'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.quantization_config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf31cc51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:11:19.789271Z",
     "iopub.status.busy": "2023-12-14T04:11:19.788577Z",
     "iopub.status.idle": "2023-12-14T04:11:19.794437Z",
     "shell.execute_reply": "2023-12-14T04:11:19.793412Z",
     "shell.execute_reply.started": "2023-12-14T04:11:19.789238Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_r = 8                  # @param {type:\"integer\"}\n",
    "lora_alpha = 32             # @param {type:\"integer\"}\n",
    "lora_dropout = 0.05         # @param {type:\"number\"}\n",
    "bias = \"none\"               # @param [\"all\", \"none\"]\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=bias,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b2a0d43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:11:22.366847Z",
     "iopub.status.busy": "2023-12-14T04:11:22.366117Z",
     "iopub.status.idle": "2023-12-14T04:11:22.372828Z",
     "shell.execute_reply": "2023-12-14T04:11:22.371708Z",
     "shell.execute_reply.started": "2023-12-14T04:11:22.366815Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @markdown ### Enter a file path:\n",
    "OUTPUT_DIR = \"experiments\"          # @param {type:\"string\"}\n",
    "\n",
    "# @markdown ---\n",
    "per_device_train_batch_size = 4     # @param {type:\"integer\"}\n",
    "gradient_accumulation_steps = 4     # @param {type:\"integer\"}\n",
    "optim = \"paged_adamw_32bit\"         # @param {type:\"string\"}\n",
    "logging_steps = 10                  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-4                # @param {type:\"number\"}\n",
    "max_grad_norm = 0.3                 # @param {type:\"number\"}\n",
    "num_train_epochs = 5                # @param {type:\"integer\"}\n",
    "evaluation_strategy = \"steps\"       # @param {type:\"string\"}\n",
    "eval_steps = 0.2                    # @param {type:\"number\"}\n",
    "warmup_ratio = 0.05                 # @param {type:\"number\"}\n",
    "save_strategy = \"epoch\"             # @param {type:\"string\"}\n",
    "lr_scheduler_type = \"cosine\"        # @param {type:\"string\"}\n",
    "fp16=True                           # @param {type:\"boolean\"}\n",
    "group_by_length = True              # @param {type:\"boolean\"}\n",
    "save_safetensors = True             # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "048c909c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:11:27.264610Z",
     "iopub.status.busy": "2023-12-14T04:11:27.263668Z",
     "iopub.status.idle": "2023-12-14T04:11:27.271915Z",
     "shell.execute_reply": "2023-12-14T04:11:27.270918Z",
     "shell.execute_reply.started": "2023-12-14T04:11:27.264573Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=fp16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    eval_steps=eval_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    save_strategy=save_strategy,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b08a7faa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:20:35.122436Z",
     "iopub.status.busy": "2023-12-14T04:20:35.122023Z",
     "iopub.status.idle": "2023-12-14T04:20:35.676830Z",
     "shell.execute_reply": "2023-12-14T04:20:35.675776Z",
     "shell.execute_reply.started": "2023-12-14T04:20:35.122404Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 79/79 [00:00<00:00, 5836.92 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 4538.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=combine_dataset[\"train\"],\n",
    "    eval_dataset=combine_dataset[\"validation\"],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=4096,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d953eb84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T04:20:40.001982Z",
     "iopub.status.busy": "2023-12-14T04:20:40.001624Z",
     "iopub.status.idle": "2023-12-14T05:58:29.562539Z",
     "shell.execute_reply": "2023-12-14T05:58:29.561632Z",
     "shell.execute_reply.started": "2023-12-14T04:20:40.001957Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6/25 00:02 < 00:10, 1.76 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.292435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConnectionError",
     "evalue": "(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: e3a1a168-0848-4b59-821b-26ca7f0650c4)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:847\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    845\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 847\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/urllib3/util/retry.py:470\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/urllib3/util/util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/transformers/trainer.py:1944\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1944\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   1947\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1948\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/transformers/trainer.py:2300\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2297\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/transformers/trainer.py:2376\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2375\u001b[0m     staging_output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2376\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstaging_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   2379\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   2380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(staging_output_dir)\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/transformers/trainer.py:2882\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   2879\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   2881\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2882\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2884\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   2885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/transformers/trainer.py:2948\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   2946\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   2947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2948\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2949\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\n\u001b[1;32m   2950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2953\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/peft/peft_model.py:216\u001b[0m, in \u001b[0;36mPeftModel.save_pretrained\u001b[0;34m(self, save_directory, safe_serialization, selected_adapters, save_embedding_layers, is_main_process, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# save only the trainable weights\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m output_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_embedding_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_embedding_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, adapter_name) \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m save_directory\n\u001b[1;32m    223\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:146\u001b[0m, in \u001b[0;36mget_peft_model_state_dict\u001b[0;34m(model, state_dict, adapter_name, unwrap_compiled, save_embedding_layers)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         has_remote_config \u001b[38;5;241m=\u001b[39m \u001b[43mfile_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (HFValidationError, EntryNotFoundError):\n\u001b[1;32m    148\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    149\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find a config file in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - will assume that the vocabulary was not modified.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         )\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/huggingface_hub/hf_api.py:2386\u001b[0m, in \u001b[0;36mHfApi.file_exists\u001b[0;34m(self, repo_id, filename, repo_type, revision, token)\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2385\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m-> 2386\u001b[0m     \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2388\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError:  \u001b[38;5;66;03m# raise specifically on gated repo\u001b[39;00m\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1628\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:408\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:67\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     69\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/requests/adapters.py:501\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[0;31mConnectionError\u001b[0m: (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: e3a1a168-0848-4b59-821b-26ca7f0650c4)')"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "072eec94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T05:58:55.827012Z",
     "iopub.status.busy": "2023-12-14T05:58:55.826631Z",
     "iopub.status.idle": "2023-12-14T05:58:55.925788Z",
     "shell.execute_reply": "2023-12-14T05:58:55.924781Z",
     "shell.execute_reply.started": "2023-12-14T05:58:55.826980Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae7660",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!zip -r /kaggle/working/experiments.zip /kaggle/working/experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54cc2fb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Inference with Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "769928f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:05:39.779813Z",
     "iopub.status.busy": "2023-12-14T06:05:39.779129Z",
     "iopub.status.idle": "2023-12-14T06:05:39.921532Z",
     "shell.execute_reply": "2023-12-14T06:05:39.920779Z",
     "shell.execute_reply.started": "2023-12-14T06:05:39.779779Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "KAGGLE_OUTPUT_DIR = \"./experiments\"\n",
    "model = PeftModel.from_pretrained(model, KAGGLE_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c615a68e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "138a1a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:05:42.178074Z",
     "iopub.status.busy": "2023-12-14T06:05:42.177673Z",
     "iopub.status.idle": "2023-12-14T06:05:42.183529Z",
     "shell.execute_reply": "2023-12-14T06:05:42.182574Z",
     "shell.execute_reply.started": "2023-12-14T06:05:42.178042Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ต๊อบชอบไปไหน\n"
     ]
    }
   ],
   "source": [
    "example = test_df.iloc[0]\n",
    "print(example.question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b352c08b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:05:57.508120Z",
     "iopub.status.busy": "2023-12-14T06:05:57.507247Z",
     "iopub.status.idle": "2023-12-14T06:09:41.573793Z",
     "shell.execute_reply": "2023-12-14T06:09:41.572773Z",
     "shell.execute_reply.started": "2023-12-14T06:05:57.508085Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 111 ms, sys: 243 ms, total: 354 ms\n",
      "Wall time: 353 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = generate_answer(model, example.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "43ba664c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:10:17.981515Z",
     "iopub.status.busy": "2023-12-14T06:10:17.980858Z",
     "iopub.status.idle": "2023-12-14T06:10:17.986442Z",
     "shell.execute_reply": "2023-12-14T06:10:17.985587Z",
     "shell.execute_reply.started": "2023-12-14T06:10:17.981484Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "308628ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:10:23.032475Z",
     "iopub.status.busy": "2023-12-14T06:10:23.032087Z",
     "iopub.status.idle": "2023-12-14T06:10:23.037497Z",
     "shell.execute_reply": "2023-12-14T06:10:23.036591Z",
     "shell.execute_reply.started": "2023-12-14T06:10:23.032444Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ชื่อต๊อบ'\n"
     ]
    }
   ],
   "source": [
    "pprint(example.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fcf624",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7b35253c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:10:29.786936Z",
     "iopub.status.busy": "2023-12-14T06:10:29.786559Z",
     "iopub.status.idle": "2023-12-14T06:10:29.792825Z",
     "shell.execute_reply": "2023-12-14T06:10:29.791719Z",
     "shell.execute_reply.started": "2023-12-14T06:10:29.786905Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ชื่ออะไร\n"
     ]
    }
   ],
   "source": [
    "example = test_df.iloc[3]\n",
    "print(example.question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7157310e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:10:31.380560Z",
     "iopub.status.busy": "2023-12-14T06:10:31.380177Z",
     "iopub.status.idle": "2023-12-14T06:10:31.934009Z",
     "shell.execute_reply": "2023-12-14T06:10:31.933042Z",
     "shell.execute_reply.started": "2023-12-14T06:10:31.380530Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 118 ms, sys: 238 ms, total: 356 ms\n",
      "Wall time: 355 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = generate_answer(model, example.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a76d309c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:10:34.678076Z",
     "iopub.status.busy": "2023-12-14T06:10:34.677702Z",
     "iopub.status.idle": "2023-12-14T06:10:34.682895Z",
     "shell.execute_reply": "2023-12-14T06:10:34.682028Z",
     "shell.execute_reply.started": "2023-12-14T06:10:34.678049Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "acf54cbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:10:36.662851Z",
     "iopub.status.busy": "2023-12-14T06:10:36.662451Z",
     "iopub.status.idle": "2023-12-14T06:10:36.668371Z",
     "shell.execute_reply": "2023-12-14T06:10:36.667311Z",
     "shell.execute_reply.started": "2023-12-14T06:10:36.662818Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ชื่อต๊อบ'\n"
     ]
    }
   ],
   "source": [
    "pprint(example.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa5b25d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b8870bd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:10:38.506826Z",
     "iopub.status.busy": "2023-12-14T06:10:38.506445Z",
     "iopub.status.idle": "2023-12-14T06:10:38.512171Z",
     "shell.execute_reply": "2023-12-14T06:10:38.511235Z",
     "shell.execute_reply.started": "2023-12-14T06:10:38.506796Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ต๊อบสูงเท่าไหร่\n"
     ]
    }
   ],
   "source": [
    "example = test_df.iloc[2]\n",
    "print(example.question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "209d84a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:10:50.772574Z",
     "iopub.status.busy": "2023-12-14T06:10:50.771879Z",
     "iopub.status.idle": "2023-12-14T06:14:14.491028Z",
     "shell.execute_reply": "2023-12-14T06:14:14.490041Z",
     "shell.execute_reply.started": "2023-12-14T06:10:50.772544Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 99.4 ms, sys: 17 ms, total: 116 ms\n",
      "Wall time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = generate_answer(model, example.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6f9a39d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:16:01.594219Z",
     "iopub.status.busy": "2023-12-14T06:16:01.593533Z",
     "iopub.status.idle": "2023-12-14T06:16:01.599236Z",
     "shell.execute_reply": "2023-12-14T06:16:01.598372Z",
     "shell.execute_reply.started": "2023-12-14T06:16:01.594187Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0732dbf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:16:03.984315Z",
     "iopub.status.busy": "2023-12-14T06:16:03.983955Z",
     "iopub.status.idle": "2023-12-14T06:16:03.989596Z",
     "shell.execute_reply": "2023-12-14T06:16:03.988781Z",
     "shell.execute_reply.started": "2023-12-14T06:16:03.984287Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'173'\n"
     ]
    }
   ],
   "source": [
    "pprint(example.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd28da5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### General Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "41b70b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:16:26.523657Z",
     "iopub.status.busy": "2023-12-14T06:16:26.523265Z",
     "iopub.status.idle": "2023-12-14T06:18:46.632132Z",
     "shell.execute_reply": "2023-12-14T06:18:46.631056Z",
     "shell.execute_reply.started": "2023-12-14T06:16:26.523626Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " 'ต๊อบชอบอะไรบ้าง\\n'\n",
      " '\\n'\n",
      " '### ต๊อกชอบกินอะไรมากที่สุด\\n'\n",
      " ' ตอบ\\n'\n",
      " 'ถั่ว\\n'\n",
      " 'น้ำ\\n'\n",
      " 'อะไรตอกกินมาก\\n'\n",
      " 'คำตอบคือต็อก\\n'\n",
      " 'คือถ๊วย\\n'\n",
      " 'ชื่อต๋๊ง\\n'\n",
      " 'ชอบถ้วยต้ม\\n'\n",
      " 'กินตับตอย\\n'\n",
      " 'ไปตากา\\n'\n",
      " 'ต่อไปต๊ะตอง\\n'\n",
      " 'ข้างต้านต้า\\n'\n",
      " 'ตามตึ๊กตี้\\n'\n",
      " 'ต่อต้วนตำติน\\n'\n",
      " 'ดังต้อตอ\\n'\n",
      " 'ก่อนตางตี\\n'\n",
      " 'หลังติงต้ง\\n'\n",
      " 'ซ้อนตงตุมต\\n'\n",
      " 'ไถตถงถ\\n'\n",
      " 'ตีตต้อยตาตายตฺตี่ตอนตือตูตานตาสตั๊ตติตรอต้อน\\n'\n",
      " 'แต่ตลกติ่งตอดตวน\\n'\n",
      " 'แต่อ้ตค๊อตโตตาร\\n'\n",
      " 'ไม่ตกลงตัวต่าต้างต่านตู้ติตุนตื่ตูกต่ายต่าวต้าวตาวตวงตามตุกตอลตัสต้องตังตอบติบตรตรีตราตริตรมตมตวะตุตักตอมตํ')\n",
      "CPU times: user 10.1 s, sys: 1.1 s, total: 11.3 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"ชื่ออะไร\"\n",
    "response = generate_answer(model, question)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b67f4c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T06:18:46.634826Z",
     "iopub.status.busy": "2023-12-14T06:18:46.634119Z",
     "iopub.status.idle": "2023-12-14T06:19:23.574131Z",
     "shell.execute_reply": "2023-12-14T06:19:23.573161Z",
     "shell.execute_reply.started": "2023-12-14T06:18:46.634786Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " 'คุณต๊อกสูง 180 เซนติเมตร\\n'\n",
      " 'ต็อบต๋อยสูงแค่ไหน\\n'\n",
      " 'เราตอกต๊ะสูง\\n'\n",
      " '\\n'\n",
      " 'คำตอบคือ: 2\\n'\n",
      " 'คือต้อต้อกระเป๋าต้งตากตางตงตองตอตตุมตอนตานตา')\n",
      "CPU times: user 3.07 s, sys: 534 ms, total: 3.6 s\n",
      "Wall time: 3.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"ต๊อบสูงเท่าไหร่\"\n",
    "response = generate_answer(model, question)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fc68d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4157536,
     "sourceId": 7190406,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30616,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-14T06:20:55.952820",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
