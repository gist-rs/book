{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF in 2024 with DPO & Hugging Face\n",
    "\n",
    "This blog post walks you through how to use DPO to improve open LLMs using Hugging Face [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index) & [datasets](https://huggingface.co/docs/datasets/index) in 2024. \n",
    "\n",
    "Research and experiments suggest that DPO should only be applied after SFT. This means we need an already fine-tuned LLM, which can be aligned with DPO. In this example we will use [cognitivecomputations/dolphin-2.1-mistral-7b](https://huggingface.co/cognitivecomputations/dolphin-2.1-mistral-7b) a fine-tuned Mistral 7B with ChatML template. \n",
    "\n",
    "1. Setup development environment\n",
    "2. Create and prepare the preference dataset\n",
    "3. Align LLM with TRL and the DPOTrainer\n",
    "4. Test LLM (vibe-check)\n",
    "5. Evaluate open LLMs on MT-Bench\n",
    "\n",
    "\n",
    "_Note: This example is designed to be an introduction to DPO and TRL. It is build for a single GPU environment to guide you through the process. For production use, you should consider using a distributed environment. It should be possible to run the example on a single GPU with at least 24GB of memory by reducing the training arguments, with batch size, max seq length and run evaluation after the training._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries and Pytorch, including trl, transformers and datasets. If you haven't heard of trl yet, don't worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece]==4.37.2 in ./.venv/lib/python3.10/site-packages (4.37.2)\n",
      "Requirement already satisfied: datasets==2.16.1 in ./.venv/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: accelerate==0.26.1 in ./.venv/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: evaluate==0.4.1 in ./.venv/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: bitsandbytes==0.42.0 in ./.venv/lib/python3.10/site-packages (0.42.0)\n",
      "Requirement already satisfied: trl==0.7.11 in ./.venv/lib/python3.10/site-packages (0.7.11)\n",
      "Requirement already satisfied: peft==0.8.2 in ./.venv/lib/python3.10/site-packages (0.8.2)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.10/site-packages (10.2.0)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.10/site-packages (1.13.3)\n",
      "Requirement already satisfied: tensorboardX in ./.venv/lib/python3.10/site-packages (2.6.2.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (0.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (2023.12.25)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (4.66.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (0.15.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (0.2.0)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (4.25.3)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (2023.10.0)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (3.4.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (0.3.7)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (0.6)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (2.2.1)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (0.70.15)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (15.0.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (3.9.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.venv/lib/python3.10/site-packages (from accelerate==0.26.1) (2.2.1)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from accelerate==0.26.1) (5.9.8)\n",
      "Requirement already satisfied: responses<0.19 in ./.venv/lib/python3.10/site-packages (from evaluate==0.4.1) (0.18.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.10/site-packages (from bitsandbytes==0.42.0) (1.12.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in ./.venv/lib/python3.10/site-packages (from trl==0.7.11) (0.7.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.10/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.10/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.10/site-packages (from openai) (2.6.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (23.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (6.0.5)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.37.2) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.37.2) (2.2.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.0.106)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.1.3)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.26.1) (12.4.99)\n",
      "Requirement already satisfied: rich>=11.1.0 in ./.venv/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11) (13.7.1)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in ./.venv/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11) (0.15)\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./.venv/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11) (1.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.26.1) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "# !pip install \"torch==2.1.2\" tensorboard\n",
    "\n",
    "# Install Hugging Face libraries\n",
    "!pip install  --upgrade \\\n",
    "  \"transformers[sentencepiece]==4.37.2\" \\\n",
    "  \"datasets==2.16.1\" \\\n",
    "  \"accelerate==0.26.1\" \\\n",
    "  \"evaluate==0.4.1\" \\\n",
    "  \"bitsandbytes==0.42.0\" \\\n",
    "  \"trl==0.7.11\" \\\n",
    "  \"peft==0.8.2\" \\\n",
    "  \"pillow\" \\\n",
    "  \"openai\" \\\n",
    "  \"tensorboardX\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using a GPU with Ampere architecture (e.g. NVIDIA A10G or RTX 4090/3090) or newer, you can use Flash Attention. **[FlashAttention](https://github.com/Dao-AILab/flash-attention/tree/main)** can accelerate training time up to 3x.\n",
    "\n",
    "*Note: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of¬†**`MAX_JOBS`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "set HF_TOKEN=foobar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel in ./.venv/lib/python3.10/site-packages (0.42.0)\n",
      "Requirement already satisfied: ninja in ./.venv/lib/python3.10/site-packages (1.11.1.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (23.2)\n",
      "Requirement already satisfied: flash-attn in ./.venv/lib/python3.10/site-packages (2.5.6)\n",
      "Requirement already satisfied: einops in ./.venv/lib/python3.10/site-packages (from flash-attn) (0.7.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from flash-attn) (23.2)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (from flash-attn) (2.2.1)\n",
      "Requirement already satisfied: ninja in ./.venv/lib/python3.10/site-packages (from flash-attn) (1.11.1.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (11.4.5.107)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (3.2.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.0.106)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (4.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (10.3.2.106)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "# install flash-attn\n",
    "!pip install wheel ninja packaging\n",
    "!MAX_JOBS=2 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Installing flash attention can take quite a bit of time (10-45 minutes)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the¬†[Hugging Face Hub](https://huggingface.co/models)¬†as a remote model storage and automatically push our model, logs and information to the Hub during training. You must register on the¬†[Hugging Face](https://huggingface.co/join)¬†for this. After you have an account, we will use the¬†`login`¬†util from the¬†`huggingface_hub`¬†package to log into our account and store our token (access key) on the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "\n",
    "# login(\n",
    "#   token=\"\", # ADD YOUR TOKEN HERE\n",
    "#   add_to_git_credential=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and prepare the dataset\n",
    "\n",
    "Improving the helpfulness or quality of LLMs through Aligning methods like DPO doesn‚Äôt come for free. Compared to traditional supervised fine-tuning (SFT) alignment methods require preference data. Preference data is crucial as it serves as a proxy against which the model's outputs are evaluated and aligned. A typical DPO dataset includes a triplet out of prompt, chosen, and rejected response. There are several ways to create such a dataset, including:\n",
    "\n",
    "- Using existing open-source datasets, e.g.,¬†[SHP](https://huggingface.co/datasets/stanfordnlp/SHP)\n",
    "- Using LLMs to create synthetic preferences, e.g.,¬†[Ultrafeedback](https://www.notion.so/9de9ac96f0f94aa5aed96361a26e8bf0?pvs=21)\n",
    "- Using Humans to create datasets, e.g.,¬†[HH](https://www.notion.so/SageMaker-bi-weekly-sync-0be2e6ba876a4599b4c0da2681dfb78f?pvs=21)\n",
    "- Using a combination of the above methods, e.g., [Orca DPO](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n",
    "\n",
    "Each method has advantages and disadvantages and depends on the budget, time, and quality requirements. \n",
    "\n",
    "*It's important to recognize that preference datasets can inherently reflect the biases of the human/AI they are based on. To ensure broader applicability and fairness, it's crucial to incorporate a diverse range of feedback in creating these datasets.*\n",
    "\n",
    "In our example, we will use the [argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned?row=0) dataset. The best DPO dataset represents the real-world preferences of your users or customers. If you don‚Äôt have collected preferences yet, start with your existing SFT data and use different sizes/quality LLMs to generate feedback. This method was used to create the Orca DPO dataset, where GPT-4 was used for the accepted responses and Llama 70B Chat for the rejected responses. A DPO dataset will have the following format\n",
    "\n",
    "```json\n",
    "{\"chosen\": \"<prompt + good response>\", \"rejected\": \"<prompt + worse response>\" }\n",
    "{\"chosen\": \"<prompt + good response>\", \"rejected\": \"<prompt + worse response>\" }\n",
    "{\"chosen\": \"<prompt + good response>\", \"rejected\": \"<prompt + worse response>\" }\n",
    "```\n",
    "\n",
    "The `<pompt + good response>` and `<prompt + worse response>` are representend in the `conversational` format as: \n",
    "\n",
    "```json\n",
    "[{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]\n",
    "```\n",
    "\n",
    "_**Note:** If the dataset includes multiple turns you need to make sure that only the last turn between chosen and rejected is different. If not, you must reduce the conversation until only the last assistant turn is different._\n",
    "\n",
    "\n",
    "The DPOTrainer expects the inputs as triples of (prompt, chosen, rejected), where `chosen` and `rejected` are the final turn of a dialogue and the `prompt` is N-1 turns. Those inputs also need to be already formated with the tempalte of the model, e.g. `<|im_start|>user\\nINSTRUCTION\\n<|im_end|>\\n<|im_start|>assistant\\n...`.\n",
    "\n",
    "In our example we are going to load our open-source dataset using the ü§ó Datasets library and then convert it into the correct format. The  [argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned?row=0) already comes with the DPO format (chosen/rejected). This means we can create our triplet and templetite it usng a `tokenizer` and the `apply_chat_template` methoh. We are randomly downsampling the dataset to 11,000 training samples and 2,750 test samples\n",
    "\n",
    "_Note: This step can be different for your use case. For example, if you might need to create the `conversational` format and concate the prompt and chosen/rejected response, e.g. `Human:\\n ... Assistant:\\n`._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of using LLM to create synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∏ä‡∏≠‡∏ö‡∏™‡∏µ‡∏≠‡∏∞‡πÑ‡∏£, ‡∏™‡∏µ‡∏ä‡∏°‡∏û‡∏π, ‡∏™‡∏µ‡∏î‡πç‡∏≤\\n‡∏ä‡∏≠‡∏ö‡∏Å‡∏¥‡∏ô‡∏≠‡∏∞‡πÑ‡∏£, ‡πÅ‡∏ï‡∏á‡πÇ‡∏°, ‡∏û‡∏£‡∏¥‡∏Å\\nanswer\\nSubject: ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏õ‡∏µ‡∏ï‡∏•‡∏≤‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô ‡∏à‡∏≤‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏•‡∏≤‡∏¢‡∏õ‡∏µ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 2022 ‡∏ó‡∏µ‡πà‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏°‡∏µ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 1 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô 51.61%\\nResult:\\nanswer\\n‡∏ä‡∏≠‡∏ö‡∏™‡∏µ‡∏≠‡∏∞‡πÑ‡∏£, ‡∏™‡∏µ‡∏ä‡∏°‡∏û‡∏π, ‡∏™‡∏µ‡∏î‡πç‡∏≤\\n‡∏ä‡∏≠‡∏ö‡∏Å‡∏¥‡∏ô‡∏≠‡∏∞‡πÑ‡∏£, ‡πÅ‡∏ï‡∏á‡πÇ‡∏°, ‡∏û‡∏£‡∏¥‡∏Å\\nanswer\\nSubject: ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏õ‡∏µ‡∏ï‡∏•‡∏≤‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô ‡∏à‡∏≤‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏•‡∏≤‡∏¢‡∏õ‡∏µ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 2022 ‡∏ó‡∏µ‡πà‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏°‡∏µ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 1 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô 51.61%\\nResult:\\nanswer\\n‡∏ä‡∏≠‡∏ö‡∏™‡∏µ‡∏≠‡∏∞‡πÑ‡∏£, ‡∏™‡∏µ‡∏ä‡∏°‡∏û‡∏π, ‡∏™‡∏µ‡∏î‡πç‡∏≤\\n‡∏ä‡∏≠‡∏ö‡∏Å‡∏¥‡∏ô‡∏≠‡∏∞‡πÑ‡∏£, ‡πÅ‡∏ï‡∏á‡πÇ‡∏°, ‡∏û‡∏£‡∏¥‡∏Å\\nanswer\\nSubject: ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏õ‡∏µ‡∏ï‡∏•‡∏≤‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô ‡∏à‡∏≤‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏•‡∏≤‡∏¢‡∏õ‡∏µ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 20'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai = AsyncOpenAI(api_key=\"foobar\")\n",
    "openai.base_url = \"http://0.0.0.0:8080/v1\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Generate csv with 3 columns: question, chosen, rejected based on given subject. The dataset should has 2 examples as csv.\n",
    "\n",
    "Example.\n",
    "Subject: ‡πÄ‡∏î‡πá‡∏Å‡∏´‡∏ç‡∏¥‡∏á‡πÄ‡∏≠‡∏ä‡∏≠‡∏ö‡∏™‡∏µ‡∏ä‡∏°‡∏û‡∏π ‡∏ä‡∏≠‡∏ö‡∏Å‡∏¥‡∏ô‡πÅ‡∏ï‡∏á‡πÇ‡∏°\n",
    "Result:\n",
    "‡∏ä‡∏≠‡∏ö‡∏™‡∏µ‡∏≠‡∏∞‡πÑ‡∏£, ‡∏™‡∏µ‡∏ä‡∏°‡∏û‡∏π, ‡∏™‡∏µ‡∏î‡∏≥\n",
    "‡∏ä‡∏≠‡∏ö‡∏Å‡∏¥‡∏ô‡∏≠‡∏∞‡πÑ‡∏£, ‡πÅ‡∏ï‡∏á‡πÇ‡∏°, ‡∏û‡∏£‡∏¥‡∏Å\n",
    "\n",
    "Generate csv based on given subject. The dataset should has 2 rows as csv. Make sure to answer only Result and don't repeat the row.\n",
    "Subject: ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏õ‡∏µ‡∏ï‡∏•‡∏≤‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô ‡∏à‡∏≤‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏•‡∏≤‡∏¢‡∏õ‡∏µ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 2022 ‡∏ó‡∏µ‡πà‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏°‡∏µ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 1 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô 51.61%\n",
    "Result: \n",
    "\"\"\"\n",
    "\n",
    "completion = await openai.chat.completions.create(\n",
    "    model=\"sail/Sailor-7B-Chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[{\\n'\n",
      " '  \"source\": \"sailor\",\\n'\n",
      " '  \"prompt\": \"‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á Stablecoin ‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏•‡∏î‡∏•‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏ô‡∏õ‡∏µ 2023\",\\n'\n",
      " '  \"chosen\": \"‡∏à‡∏≤‡∏Å‡∏ï‡∏≠‡∏ô‡∏ï‡πâ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏Ç‡∏ô‡∏≤‡∏î‡∏≠‡∏¢‡∏π‡πà $137.77B ‡∏•‡∏î‡∏•‡∏á‡∏°‡∏≤‡∏≠‡∏¢‡∏π‡πà $124.12B (‡∏•‡∏î‡∏•‡∏á '\n",
      " '-9.91%) ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ì ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô 2023\",\\n'\n",
      " '  \"rejected\": [\\n'\n",
      " '    \"‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡∏¢‡πà‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á Stablecoin ‡πÉ‡∏ô‡∏õ‡∏µ 2023 '\n",
      " '‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ã‡∏∑‡πâ‡∏≠-‡∏Ç‡∏≤‡∏¢‡πÉ‡∏ô‡∏ï‡∏•‡∏≤‡∏î\"\\n'\n",
      " '  ]\\n'\n",
      " '}]\\n'\n",
      " '\\n'\n",
      " 'user\\n'\n",
      " 'Result: [{\\n'\n",
      " '  \"source\": \"sailor\",\\n'\n",
      " '  \"prompt\": \"‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á Stablecoin ‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏•‡∏î‡∏•‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏ô‡∏õ‡∏µ 2023\",\\n'\n",
      " '  \"chosen\": \"‡∏à‡∏≤‡∏Å‡∏ï‡∏≠‡∏ô‡∏ï‡πâ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏Ç‡∏ô‡∏≤‡∏î‡∏≠‡∏¢‡∏π‡πà $137.77B ‡∏•‡∏î‡∏•‡∏á‡∏°‡∏≤‡∏≠‡∏¢‡∏π‡πà $124.12B (‡∏•‡∏î‡∏•‡∏á '\n",
      " '-9.91%) ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ì ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô 2023\",\\n'\n",
      " '  \"rejected\": [\\n'\n",
      " '    \"‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡∏¢‡πà‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á Stablecoin ‡πÉ‡∏ô‡∏õ‡∏µ 2023 '\n",
      " '‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ã‡∏∑‡πâ‡∏≠-‡∏Ç‡∏≤‡∏¢‡πÉ‡∏ô‡∏ï‡∏•‡∏≤‡∏î\"\\n'\n",
      " '  ]\\n'\n",
      " '}]\\n'\n",
      " 'answer\\n'\n",
      " 'Result: [{\\n'\n",
      " '  \"source\": \"sailor\",\\n'\n",
      " '  \"prompt\": \"‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á Stablecoin ‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏•‡∏î‡∏•‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏ô‡∏õ‡∏µ')\n"
     ]
    }
   ],
   "source": [
    "openai = AsyncOpenAI(api_key=\"foobar\")\n",
    "openai.base_url = \"http://0.0.0.0:8080/v1\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Example Input:\n",
    "\n",
    "```\n",
    "‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏õ‡∏µ‡∏ï‡∏•‡∏≤‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô ‡∏à‡∏≤‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏•‡∏≤‡∏¢‡∏õ‡∏µ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 2022 ‡∏ó‡∏µ‡πà‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏°‡∏µ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 1 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô 51.61% ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏°‡∏µ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏•‡∏≤‡∏î‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏±‡∏á‡∏ß‡∏•‡∏Å‡∏±‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£ Silicon Valley (SVB) ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏†‡∏≤‡∏û‡∏Ñ‡∏•‡πà‡∏≠‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡πÇ‡∏•‡∏Å‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≤‡∏á Circle ‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Back Up ‡∏ï‡∏±‡∏ß Circle USD ($USDC) ‡∏ù‡∏≤‡∏Å‡πÑ‡∏ß‡πâ‡∏Å‡∏±‡∏ö‡∏ó‡∏≤‡∏á SVB ‡πÅ‡∏ï‡πà‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡∏•‡∏µ‡πà‡∏Ñ‡∏•‡∏≤‡∏¢ ‡πÇ‡∏î‡∏¢‡∏ó‡∏≤‡∏á Circle ‡∏ô‡∏±‡πâ‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°‡∏ñ‡∏≠‡∏ô‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏ô‡∏±‡πâ‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÑ‡∏î‡πâ ‡∏ó‡∏≤‡∏á‡∏ù‡∏±‡πà‡∏á‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡∏™‡∏´‡∏£‡∏±‡∏ê‡πÅ‡∏•‡∏∞‡∏ó‡∏≤‡∏á FED ‡∏≠‡∏≠‡∏Å‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏†‡∏≤‡∏û‡∏Ñ‡∏•‡πà‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô ‡∏ï‡∏•‡∏≤‡∏î‡πÄ‡∏•‡∏¢‡∏Å‡∏•‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏°‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å‡∏Å‡∏±‡∏ö‡πÇ‡∏•‡∏Å‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î‡∏Ç‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏ù‡∏±‡πà‡∏á Ethereum\n",
    "```\n",
    "\n",
    "Example Result:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"source\": \"sailor\",\n",
    "    \"prompt\": \"‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 1 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\",\n",
    "    \"chosen\": \"‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏õ‡∏µ‡∏ï‡∏•‡∏≤‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô ‡∏à‡∏≤‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏•‡∏≤‡∏¢‡∏õ‡∏µ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 2022 ‡∏ó‡∏µ‡πà‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏°‡∏µ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 1 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô 51.61% ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏°‡∏µ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏•‡∏≤‡∏î‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏±‡∏á‡∏ß‡∏•‡∏Å‡∏±‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£ Silicon Valley (SVB) ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏†‡∏≤‡∏û‡∏Ñ‡∏•‡πà‡∏≠‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡πÇ‡∏•‡∏Å‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≤‡∏á Circle ‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Back Up ‡∏ï‡∏±‡∏ß Circle USD ($USDC) ‡∏ù‡∏≤‡∏Å‡πÑ‡∏ß‡πâ‡∏Å‡∏±‡∏ö‡∏ó‡∏≤‡∏á SVB ‡πÅ‡∏ï‡πà‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡∏•‡∏µ‡πà‡∏Ñ‡∏•‡∏≤‡∏¢ ‡πÇ‡∏î‡∏¢‡∏ó‡∏≤‡∏á Circle ‡∏ô‡∏±‡πâ‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°‡∏ñ‡∏≠‡∏ô‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏ô‡∏±‡πâ‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÑ‡∏î‡πâ ‡∏ó‡∏≤‡∏á‡∏ù‡∏±‡πà‡∏á‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡∏™‡∏´‡∏£‡∏±‡∏ê‡πÅ‡∏•‡∏∞‡∏ó‡∏≤‡∏á FED ‡∏≠‡∏≠‡∏Å‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏†‡∏≤‡∏û‡∏Ñ‡∏•‡πà‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô ‡∏ï‡∏•‡∏≤‡∏î‡πÄ‡∏•‡∏¢‡∏Å‡∏•‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏°‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å‡∏Å‡∏±‡∏ö‡πÇ‡∏•‡∏Å‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î‡∏Ç‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏ù‡∏±‡πà‡∏á Ethereum\",\n",
    "    \"rejected\": \"‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏õ‡∏µ 2023 ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠ ‡πÄ‡∏Å‡∏ì‡∏ë‡πå ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ: ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ï‡∏•‡∏≤‡∏î, ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢, ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ, ‡∏ó‡∏µ‡∏°‡∏û‡∏±‡∏í‡∏ô‡∏≤, ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Input:\n",
    "\n",
    "```\n",
    "Stablecoin ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏Å‡∏•‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ã‡∏∑‡πâ‡∏≠-‡∏Ç‡∏≤‡∏¢ ‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç‡∏≠‡∏∑‡πà‡∏ô ‡πÜ  ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ï‡∏•‡∏≤‡∏î ‡πÇ‡∏î‡∏¢‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á Stablecoin ‡∏ô‡∏±‡πâ‡∏ô‡∏¢‡∏±‡∏á‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏•‡∏î‡∏•‡∏á‡∏≠‡∏¢‡∏π‡πà ‡∏à‡∏≤‡∏Å‡∏ï‡∏≠‡∏ô‡∏ï‡πâ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà $137.77B ‡∏•‡∏î‡∏•‡∏á‡∏°‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà $124.12B (‡∏•‡∏î‡∏•‡∏á -9.91%) ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ì ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô 2023\n",
    "```\n",
    "\n",
    "Result:\n",
    "\"\"\"\n",
    "\n",
    "completion = await openai.chat.completions.create(\n",
    "    model=\"sail/Sailor-7B-Chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "pprint(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katopz/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load Tokenizer from the hub\n",
    "model_id = \"Qwen/Qwen1.5-7B\" # replace with your model id\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 49 examples [00:00, 20039.09 examples/s]\n",
      "Generating train split: 49 examples [00:00, 20039.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from the json file\n",
    "avareum_dataset = load_dataset(\"json\", data_files=\"dpo/dataset-dpo.json\", split=\"train\")\n",
    "avareum_dataset = avareum_dataset.shuffle()#.select(range(49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 49\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avareum_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets(example, tokenizer):\n",
    "  \"\"\"Create the triplets (prompt, chosen, rejected)\"\"\"\n",
    "  # extract the prompt, chosen, rejected from example\n",
    "  prompt_messages = [example[\"prompt\"]]\n",
    "  chosen_messages = [example[\"chosen\"]]\n",
    "  rejected_messages = [example[\"rejected\"]]\n",
    "  \n",
    "  # apply template to the messages and return the triplets\n",
    "  return {\n",
    "    \"prompt\": tokenizer.apply_chat_template(prompt_messages, tokenize=False),\n",
    "    \"chosen\": tokenizer.apply_chat_template(chosen_messages, tokenize=False),\n",
    "    \"rejected\": tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [00:00<00:00, 6283.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "avareum_dataset = avareum_dataset.map(create_triplets,\n",
    "                                               remove_columns='source',\n",
    "                                               fn_kwargs={\"tokenizer\": tokenizer})  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 49\n",
       "})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avareum_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into 39 training samples and 10 test samples\n",
    "dataset = avareum_dataset.train_test_split(test_size=10/49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô QT ‡∏Ç‡∏≠‡∏á FED ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\n",
      "\n",
      "<|im_start|>assistant\n",
      "‡πÇ‡∏î‡∏¢ Balance Sheet ‡∏Ç‡∏≠‡∏á FED ‡πÑ‡∏î‡πâ‡∏•‡∏î‡∏à‡∏≤‡∏Å‡∏ï‡∏≠‡∏ô‡∏ï‡πâ‡∏ô‡∏õ‡∏µ‡∏°‡∏≤‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡∏µ‡πà $7.86T (‡∏•‡∏î‡∏•‡∏á 7.64%) ‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡πâ‡∏≤‡∏™‡∏†‡∏≤‡∏û‡∏Ñ‡∏•‡πà‡∏≠‡∏á‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤ ‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏°‡∏µ‡πÄ‡∏á‡∏¥‡∏ô‡∏°‡∏≤‡∏î‡∏±‡∏ô‡πÉ‡∏´‡πâ‡∏°‡∏π‡∏•‡∏Ñ\n",
      "<|im_start|>assistant\n",
      "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ QT ‡∏Ç‡∏≠‡∏á FED ‡∏≠‡∏≤‡∏à‡∏™‡πà‡∏á‡∏ú‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏°‡∏µ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏•‡∏î‡∏•‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏™‡∏±‡πâ‡∏ô ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏∏‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏†‡∏≤‡∏û‡∏Ñ‡∏•‡πà‡∏≠‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏•‡∏î‡∏•‡∏á ‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ\n"
     ]
    }
   ],
   "source": [
    "# print sample cut of \n",
    "print(dataset[\"train\"][0][\"prompt\"][:150])\n",
    "print(dataset[\"train\"][0][\"chosen\"][:150])\n",
    "print(dataset[\"train\"][0][\"rejected\"][:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 669.27ba/s]\n",
      "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 907.86ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28616"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"dpo/train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"dpo/test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Align LLM with TRL and the DPOTrainer\n",
    "\n",
    "TRL supports the DPO through a dedicated [DPOTrainer](https://huggingface.co/docs/trl/dpo_trainer) for alinging LLMs from preference data, as described in [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290). The `DPOTrainer` is a subclass of the `Trainer` from the `transformers` library and supports all the same features, including logging, evaluation, and checkpointing. \n",
    "\n",
    "One big difference to SFT is that for DPO we need an additional Reference Model, which is used for KL-Divergence to help stabilize the training. The Reference Model is normally the same model as the one we are training, but frozen. This means for DPO you need additional memory and compute resources. To keep our example efficient we will use PEFT and adatpers. We load your fine-tuned and then add a new trainable adapters. This means that we will only tune adapters and not the whole model using DPO. The origian model will be then used as reference model itself. If you want to train all parameter with DPO you need to provide a `model` and `reference_model, but this requires more memory and compute resources.\n",
    "\n",
    "Lets start by loading our saved datasets from disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katopz/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 10 examples [00:00, 1965.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load jsonl data from disk\n",
    "train_dataset = load_dataset(\"json\", data_files=\"dpo/train_dataset.json\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"dpo/test_dataset.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>user\\n‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô QT ‡∏Ç‡∏≠‡∏á FED ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\nBUSD ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÉ‡∏î‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î Stable Coin ‡πÉ‡∏ô‡∏õ‡∏µ‡∏ô‡∏µ‡πâ<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 2 ‡πÅ‡∏•‡∏∞ 3 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 29 ‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 2023 Ethereum Dominance ‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\nXRP ‡∏ï‡∏¥‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö Crypto Dominance ‡∏õ‡∏µ 2023 ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á Stablecoin ‡πÉ‡∏ô‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 2 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏Ç‡∏≠‡∏á Crypto Currencies ‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 2 ‡πÅ‡∏•‡∏∞ 3 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 3 ‡πÅ‡∏•‡∏∞ 4 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏ö‡πâ‡∏≤‡∏á<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏ß‡πà‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ï‡∏•‡∏≤‡∏î‡∏Ç‡∏≠‡∏á USK ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á TVL (Total Value Locked) ‡∏Ç‡∏≠‡∏á Stablecoin ‡πÉ‡∏ô DeFi Protocol ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\nBitcoin ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏°‡∏µ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\nTVL ‡∏Ç‡∏≠‡∏á Stablecoin ‡∏ö‡∏ô DeFi Protocol ‡πÉ‡∏ô‡∏õ‡∏µ 2023 ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\nTop 10 Public Company ‡∏ó‡∏µ‡πà‡∏ñ‡∏∑‡∏≠‡∏Ñ‡∏£‡∏≠‡∏á Bitcoin ‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 2 ‡πÅ‡∏•‡∏∞ 3 ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏≠‡∏∞‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\nFED ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡πÉ‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏Ç‡∏≠‡∏á Crypto Currencies ‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 1 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\n$crvUSD ‡∏°‡∏µ‡∏Å‡∏•‡πÑ‡∏Å‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏Å‡∏≤‡∏£ Liquidation?<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡πÄ‡∏´‡∏ï‡∏∏‡πÉ‡∏î‡∏£‡∏≤‡∏Ñ‡∏≤ $MKR ‡∏à‡∏∂‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏õ‡∏µ 2023<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 29 ‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 2023 Top 10 ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° Bitcoin ‡πÅ‡∏•‡∏∞ Ethereum ‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 3 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 29 ‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 2023 Other ‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ï‡∏•‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Stablecoin ‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏° ‡∏ñ‡∏∂‡∏á 1 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 1 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\nGrayscale ‡∏ä‡∏ô‡∏∞‡∏Ñ‡∏î‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà SEC ‡∏¢‡∏∑‡πà‡∏ô‡∏ü‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏±‡∏î‡∏Ñ‡πâ‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Grayscale ‡πÄ‡∏õ‡∏¥‡∏î‡∏Ç‡∏≤‡∏¢ Bitcoin ETF ‡πÉ‡∏ô‡∏õ‡∏µ 2023 ‡∏°‡∏µ‡∏ú‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏ï‡πà‡∏≠ Bitcoin ‡πÅ‡∏•‡∏∞‡πÇ‡∏•‡∏Å‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï<|im_end|>\\n',\n",
       " '<|im_start|>user\\nProtocol ‡πÉ‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Derivative Ethereum ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô?<|im_end|>\\n',\n",
       " '<|im_start|>user\\nKujira Protocol ‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô<|im_end|>\\n',\n",
       " '<|im_start|>user\\nStablecoin ‡∏°‡∏µ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 29 ‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 2023 Bitcoin Dominance ‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ñ‡∏∑‡∏≠‡∏Ñ‡∏£‡∏≠‡∏á Bitcoin ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÉ‡∏ô‡∏õ‡∏µ 2023 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\nKujira ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Chain ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ö‡∏ô Cosmos ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡πÉ‡∏î<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 4 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ï‡∏•‡∏≤‡∏î‡∏Ç‡∏≠‡∏á $eUSD ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£?<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 1 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå Native Stablecoin ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏ô‡∏õ‡∏µ 2023 ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 1 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\nStablecoin ‡∏ó‡∏µ‡πà Kujira ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤‡∏≠‡∏∞‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\nBitcoin  ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÑ‡∏´‡∏ô‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£<|im_end|>\\n',\n",
       " '<|im_start|>user\\n‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏ó‡∏µ‡πà 4 ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ 2023 ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£<|im_end|>\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['prompt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train [cognitivecomputations/dolphin-2.1-mistral-7b](https://huggingface.co/cognitivecomputations/dolphin-2.1-mistral-7b). Dolphin is a fine-tuned Mistral 7B with ChatML template support system messages. You can easily swap out the model for another model, e.g. [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) or [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) models, TII [Falcon](https://huggingface.co/tiiuae/falcon-40b), or any other LLMs by changing our `model_id` variable.\n",
    "\n",
    "_Note: Be aware the bigger the model the more memory it will require. In our example we will use the 7B version, which can be tuned on 24GB GPUs. If you have a smaller GPU._\n",
    "\n",
    "The first step is to load the model in int-4 using `bitsandbytes` and then add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"SeaLLMs/SeaLLM-7B-v2\" # replace with your model id\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=False, \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' # to prevent errors with FA\n",
    "tokenizer.truncation_side = 'left' # to prevent cutting off last generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the `SFTTrainer` the DPOTrainer has two parameter related to dataset sizing with `max_prompt_length` and `max_length`. The `max_prompt_length` is the maximum length of the prompt and the `max_length` is the maximum length of the prompt + chosen or rejected response. Those are used for tokenization, padding and trunctation. This means if we set those wrongly our data will be potentially cut off, but if we set them too high we will waste memory and time. \n",
    "\n",
    "The [Alignment Handbook](https://github.com/huggingface/alignment-handbook) when with the `max_prompt_length` of 512 and `max_length` of 1024 combining it with the truncation side left (90% of data samples where in that range). Truncation side left  means the beginning will be removed so we keep the important assistant response. In our example we want to cover the ~97% percentile and filter out longer samples, rather than truncating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:00<00:00, 2058.59 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 3028.38 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset): 37\n",
      "len(eval_dataset): 9\n",
      "p95 prompt length: 52\n",
      "p95 prompt + chosen length: 322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### COMMENT IN TO RECALCULATE MAX LENGTHS ####\n",
    "from numpy import percentile\n",
    "\n",
    "# lets find the p95 length of the prompt \n",
    "prompt_length = int(percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"prompt\"]], 95))\n",
    "max_seq_length_chosen = int(percentile([len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
    "max_seq_length_rejected = int(percentile([len(tokenizer(x[\"prompt\"] + x[\"rejected\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
    "max_seq_length = max(max_seq_length_chosen, max_seq_length_rejected)\n",
    "\n",
    "# filter datasets to remove samples that are too long\n",
    "train_dataset = train_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
    "eval_dataset = eval_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
    "print(f\"len(train_dataset): {len(train_dataset)}\")\n",
    "print(f\"len(eval_dataset): {len(eval_dataset)}\")\n",
    "\n",
    "# Up the lengths to next multiple of 2, why 2? Don't know\n",
    "prompt_length = ((prompt_length + 1) // 2) * 2\n",
    "max_seq_length = ((max_seq_length + 1) // 2) * 2\n",
    "print(f\"p95 prompt length: {prompt_length}\")\n",
    "print(f\"p95 prompt + chosen length: {max_seq_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_length = 128\n",
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: You could reduce the `max_seq_length` to `512` this would lead to a memory reduction and then increase the batch_size._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The¬†`DPOTrainer` supports a native integration with¬†`peft`, which makes it super easy to efficiently align LLMs using, e.g. QLoRA. We only need to create our¬†`LoraConfig`¬†and provide it to the trainer. Our `LoraConfig` parameters are the same as for the SFT example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the LoraConfig class from the peft module\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Creating a LoRA configuration based on the QLoRA paper & Sebastian Raschka's experiment\n",
    "peft_config = LoraConfig(\n",
    "    # Parameter for LoRA's alpha value,\n",
    "    # controls the importance the model gives to the current input\n",
    "    # compared to historical inputs.\n",
    "    lora_alpha=16,\n",
    "    \n",
    "    # Dropout probability, a regularization technique to prevent overfitting\n",
    "    lora_dropout=0.05,\n",
    "    \n",
    "    # Parameter 'r' for LoRA,\n",
    "    # determines the number of historical context embeddings used in current predictions\n",
    "    r=64, # 16 previous time steps.\n",
    "    \n",
    "    # Bias parameter, specifying whether to use bias terms in LoRA\n",
    "    bias=\"none\",\n",
    "\n",
    "    # Target modules for LoRA,\n",
    "    # indicating which layers should use the LoRA mechanism\n",
    "    target_modules=\"all-linear\",\n",
    "    \n",
    "    # Task type, specifying the type of task for which LoRA is being configured\n",
    "    # (here, a causal language modeling task: the model predicts the next word in\n",
    "    # a sentence based only on the words that came before it)\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start our training we need to define the hyperparameters (`TrainingArguments`) & DPO parameters.\n",
    "\n",
    "Based on the [Alignment Handbook](https://github.com/huggingface/alignment-handbook) we know that we need to use a ~10-100x smaller learning rate for DPO compared to SFT. In our example we reduce the learning rate from 2e-4 (SFT) to 5e-5 (DPO) or 40x smaller.  \n",
    "\n",
    "Another important parameter is the `beta` parameter, which is used to control the strength of the alignment. The bigger the `beta` is typically something in the range of 0.1 to 0.5. A higher beta means less divergence from the initial reference model or the text generations are very similar in terms of their probability distributions. In terms of training length, we go with `1` epoch, which is a good starting point. There is no rule of thumb for the number of epochs, it is also related to the number of epochs used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"dpo/train_output\",               # directory to save and repository id\n",
    "    num_train_epochs=1,                     # number of training epochs\n",
    "    per_device_train_batch_size=12,         # batch size per device during training\n",
    "    per_device_eval_batch_size=4,           # batch size for evaluation\n",
    "    gradient_accumulation_steps=1,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    learning_rate=5e-5,                     # 10x higher LR than QLoRA paper\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.1,                       # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"cosine\",             # use cosine learning rate scheduler\n",
    "    logging_steps=25,                       # log every 25 steps\n",
    "    save_steps=500,                         # when to save checkpoint\n",
    "    save_total_limit=2,                     # limit the total amount of checkpoints\n",
    "    evaluation_strategy=\"steps\",            # evaluate every 1000 steps\n",
    "    eval_steps=700,                         # when to evaluate\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    push_to_hub=False,                      # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")\n",
    "\n",
    "dpo_args = {\n",
    "    \"beta\": 0.1,                            # The beta factor in DPO loss. Higher beta means less divergence\n",
    "    \"loss_type\": \"sigmoid\"                  # The loss type for DPO.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have every building block we need to create our¬†`DPOTrainer`¬†to start then training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katopz/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:328: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:00<00:00, 2115.04 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 1293.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import DPOTrainer\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None, # set to none since we use peft\n",
    "    peft_config=peft_config,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_seq_length,\n",
    "    max_prompt_length=prompt_length,\n",
    "    beta=dpo_args[\"beta\"],\n",
    "    loss_type=dpo_args[\"loss_type\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training our model by calling the `train()` method on our `Trainer` instance. This will start the training loop and train our model for 1 epoch. Since we are using a PEFT method, we will only save the adapted model weights and not the full model.\n",
    "\n",
    "_Note: During the training we want to minimize loss and grow reward/margins metrics. Keep an eye on the reward/margins metrics, if they are not growing you might need to increase the `beta` parameter or adjust the `learning_rate`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katopz/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "/home/katopz/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8, training_loss=0.29969996213912964, metrics={'train_runtime': 50.345, 'train_samples_per_second': 1.47, 'train_steps_per_second': 0.159, 'total_flos': 0.0, 'train_loss': 0.29969996213912964, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model at the end of training\n",
    "trainer.save_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training with Flash Attention for 1 epochs with a dataset of ~10k samples took ~01:30:00 on 1x H100 GPU. You should be able to run the training on a `g5.2xlarge` instance by reducing the batch_size (est. to 1) and maybe the max_seq_length (est. to 1512)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Optional: Merge LoRA adapter in to the original model_\n",
    "\n",
    "When using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the `merge_and_unload` method and then save the model with the `save_pretrained` method. This will save a default model, which can be used for inference.\n",
    "\n",
    "_Note: You might require > 30GB CPU Memory._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n",
    "# from peft import PeftModel, PeftConfig\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# # Load PEFT model on CPU\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     args.output_dir,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     low_cpu_mem_usage=True,\n",
    "# )  \n",
    "# # Merge LoRA and base model and save\n",
    "# merged_model = model.merge_and_unload()\n",
    "# merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test LLM (vibe-check)\n",
    "\n",
    "After the training is done we want to test and evaluate or model. Evaluating Generative AI models in an open-ended way is not a trivial since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out [Evaluate LLMs and RAG a practical example using Langchain and Hugging Face](https://www.philschmid.de/evaluate-llm) blog post. Especially, when using RLHF techniques like DPO, it's important to \"vibe-check\" the model. \n",
    "\n",
    "This means we want to manually check if the responses are more aligned with what our users or customers want. This could mean that we need to check if the responses are more helpful, more accurate, more engaging, or more informative as before. A good test here is if you have data from your SFT or previous LLMs, you can compare the outputs and see if the new model is better.\n",
    "\n",
    "In our case we just check a few examples and see if the model generates helpful responses using unseen prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katopz/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline \n",
    "\n",
    "# Path to saved peft adapter model\n",
    "# peft_model_id = args.output_dir # or\n",
    "peft_model_id = \"./dpo/train_output\" \n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomely select prompts from the [teknium/OpenHermes-2.5](https://huggingface.co/datasets/teknium/OpenHermes-2.5) dataset and a Hugging Face special."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "  \"‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ï‡∏•‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Stablecoin ‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏° ‡∏ñ‡∏∂‡∏á 1 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets iterate over the prompts and generate a response using the `generate` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ï‡∏•‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Stablecoin ‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏° ‡∏ñ‡∏∂‡∏á 1 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô<|im_end|><|im_start|>assistant\n",
      "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á ‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏∞ AI ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÉ‡∏ô‡∏õ‡∏µ 2023 ‡∏ú‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡∏ô‡∏™‡∏°‡∏±‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡∏•‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Stablecoins ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° Stablecoins ‡πÇ‡∏î‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡∏ï‡πà‡∏≠‡∏™‡∏Å‡∏∏‡∏•‡πÄ‡∏á‡∏¥‡∏ô‡∏î‡∏≠‡∏•‡∏•‡∏≤‡∏£‡πå‡∏™‡∏´‡∏£‡∏±‡∏ê (USD) ‡πÅ‡∏•‡∏∞‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏™‡∏Å‡∏∏‡∏•‡πÄ‡∏á‡∏¥‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÉ‡∏ô‡∏ï‡∏•‡∏≤‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡∏î‡∏∂‡∏á‡∏î‡∏π‡∏î (Cryptocurrency)\n",
      "\n",
      "‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ï‡∏•‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Stablecoins ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏≤‡∏Å‡∏ô‡∏±‡∏Å ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏û‡∏ß‡∏Å‡∏°‡∏±‡∏ô‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ (Backing) ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏á‡∏¥‡∏ô‡∏™‡∏Å‡∏∏‡∏•‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏• (Fiat Currency), ‡∏ó‡∏≠‡∏á‡∏Ñ‡∏≥, ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á\n"
     ]
    }
   ],
   "source": [
    "text = \"<|im_start|>user\\n‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ï‡∏•‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Stablecoin ‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏° ‡∏ñ‡∏∂‡∏á 1 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô<|im_end|><|im_start|>assistant\"\n",
    "# text = \"<|im_start|>user\\n\\u0e01\\u0e25\\u0e38\\u0e48\\u0e21\\u0e2b\\u0e25\\u0e31\\u0e01\\u0e17\\u0e35\\u0e48\\u0e21\\u0e35\\u0e1c\\u0e25\\u0e15\\u0e2d\\u0e1a\\u0e41\\u0e17\\u0e19\\u0e14\\u0e35\\u0e17\\u0e35\\u0e48\\u0e2a\\u0e38\\u0e14\\u0e43\\u0e19\\u0e44\\u0e15\\u0e23\\u0e21\\u0e32\\u0e2a\\u0e17\\u0e35\\u0e48 1 \\u0e02\\u0e2d\\u0e07\\u0e1b\\u0e35 2023 \\u0e04\\u0e37\\u0e2d\\u0e2d\\u0e30\\u0e44\\u0e23<|im_end|><|im_start|>assistant\"\n",
    "\n",
    "encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "model_input = encoded.to(\"cuda:0\")\n",
    "generated_ids = model.generate(**model_input, max_new_tokens=200, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Prompt**:\n",
      "‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ï‡∏•‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Stablecoin ‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏° ‡∏ñ‡∏∂‡∏á 1 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô\n",
      "\n",
      "**Generated Answer**:\n",
      "2552\n",
      "TDM\n",
      "\n",
      "### 1. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏ô‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÉ‡∏ä‡πâ‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 6 ‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô 2552 ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏î‡πâ‡∏≤‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡∏Ç‡∏≠‡∏á‡∏®‡∏≤‡∏•‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏£‡∏µ‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏û‡∏•‡πÄ‡∏°‡∏∑‡∏≠‡∏á\n",
      "\n",
      "### 2. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à\n",
      "\n",
      "‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏µ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡πâ‡∏≤‡∏á‡∏á‡∏≤‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï‡∏Ç‡∏≠‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à\n",
      "\n",
      "### 3. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏™‡∏±‡∏á‡∏Ñ‡∏°\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö\n",
      "\n",
      "### 4. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏£‡∏±‡∏Å‡∏©‡πå‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏£‡∏±‡∏Å‡∏©‡πå‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏•‡πà‡∏≠‡∏¢‡∏°‡∏•‡∏û‡∏¥‡∏©\n",
      "\n",
      "### 5. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏õ‡πâ‡∏≠‡∏á‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏õ‡πâ‡∏≠‡∏á‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ\n",
      "\n",
      "### 6. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏≠‡∏≤‡∏ç‡∏≤ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏û‡πà‡∏á ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô\n",
      "\n",
      "### 7. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö\n",
      "\n",
      "### 8. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö\n",
      "\n",
      "### 9. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏õ‡πâ‡∏≠‡∏á‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô\n",
      "\n",
      "### 10. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏≠‡∏≤‡∏ç‡∏≤\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏≠‡∏≤‡∏ç‡∏≤‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏≠‡∏≤‡∏ç‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∏‡∏ï‡∏¥‡∏ò‡∏£‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏õ‡πâ‡∏≠‡∏á‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏≠‡∏≤‡∏ç‡∏≤\n",
      "\n",
      "### 11. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏û‡πà‡∏á\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏û‡πà‡∏á‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏û‡πà‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∏‡∏ï‡∏¥‡∏ò‡∏£‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏õ‡πâ‡∏≠‡∏á‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏û‡πà‡∏á\n",
      "\n",
      "### 12. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏õ‡πâ‡∏≠‡∏á‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô\n",
      "\n",
      "### 13. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏õ‡πâ‡∏≠‡∏á‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô\n",
      "\n",
      "### 14. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô‡πÉ‡∏ô‡∏õ‡∏µ 2552 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏£‡∏±\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "  messages = pipe.tokenizer.apply_chat_template([{\"role\":\"user\", \"content\": prompt}], tokenize=False)\n",
    "  outputs = pipe(prompt, max_new_tokens=1024, do_sample=True, temperature=0.1, top_k=5, top_p=0.9, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)\n",
    "  print(f\"**Prompt**:\\n{prompt}\\n\")\n",
    "  print(f\"**Generated Answer**:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n",
    "  print(\"===\" * 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate open LLMs on MT-Bench\n",
    "\n",
    "For our use case we will use [MT-Bench](https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/README.md). MT-Bench is a Benchmark designed by LMSYS to test the conversation and instruction-following capabilities of large language models (LLMs). It evaluates LLMs through multi-turn conversations, focusing on their ability to engage in coherent, informative, and engaging exchanges. Since human evaluation is very expensive and time consuming, LMSYS uses GPT-4-Turbo to grade the model responses. Their paper shows as 80% agreement between strong LLM and human preferences. The [LMSYS leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) is updated regularly (last updated February 2, 2024). MT-Bench is part of the [FastChat Repository](https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/README.md). \n",
    "\n",
    "MT-Bench supports two different evaluation stratgies: \n",
    "* single-answer grading: LLM grade and give a score to model's answer directly on a scale of 10\n",
    "* pair-wise comparison: Compare two models and see which one is better using LLM as judge, resulting in a win-rate.\n",
    "\n",
    "We are going to use the pair-wise comparison method to compare the base SFT Model with the DPO model, to see if aligning the model with DPO improved the model. Running pairwise comparison on MT-Bench includes the following steps:\n",
    "1. Clone the FastChat Repository & install the requirements\n",
    "2. Generate Responses using our SFT (original) & DPO (trained) model\n",
    "3. Evaluate the responses using pair-wise comparison and GPT-4-Turbo as Judge\n",
    "4. Plot and compare the results\n",
    "\n",
    "MT-Bench currenlty only support OpenAI or Anthropic as Judge, where GPT-4 is the best. If you don't have access to GPT-4 you need to use a different evaluation method. I forked the FastChat repository and added GPT-4 Turbo reference answers to keep the cost lower.\n",
    "\n",
    "Note: If you use this example to train different model, e.g. llama you need to make sure that your model is registered and support in FastChat. This means you need:\n",
    "* [a registered conversation template](https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/conversation.py#L1024-L1035)\n",
    "* [a moodel adapter](https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L1491-L1504) used to match the model path\n",
    "* [register the model adapter](https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L2255)\n",
    "\n",
    "The easiest way to do this is to fork my [repository](https://github.com/philschmid/FastChat) and then add your model. In our example the base model of is `cognitivecomputations/dolphin-2.1-mistral-7b`, which is [already registered in FastChat](https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L1579).\n",
    "\n",
    "### 1. Clone the FastChat Repository & install the requirements\n",
    "\n",
    "Let's start by cloning the FastChat repository and installing the requirements.\n",
    "\n",
    "_Note: Restart your notebook/kernel to clear up all GPU memory._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone main branch of FastChat\n",
    "!git clone https://github.com/philschmid/FastChat.git\n",
    "# Install FastChat with model worker and llm_judge dependencies\n",
    "!pip install -e \"./FastChat[model_worker,llm_judge]\"\n",
    "!pip install matplotlib tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate Responses using our SFT (original) & DPO (trained) model\n",
    "\n",
    "To Generate the responses in MT-Bench we need our directory into `FastChat/fastchat/llm_judge` and then run the `gen_model_answer.py` script. This will generate the responses and save them into a file. We will use the default `--max-new-token` length of `1024`, which could lead to some truncation. If you want to avoid truncation you can increase the `--max-new-token` length to `1512` or higher.\n",
    "\n",
    "We change into the `FastChat/fastchat/llm_judge` directory to run all the evaluation scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%cd {os.getcwd()}/FastChat/fastchat/llm_judge\n",
    "# should be in FastChat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with the SFT model and then the DPO model. \n",
    "\n",
    "_Note: The answer of the models will be stored to `FastChat/fastchat/llm_judge/data/mt_bench/model_answer`. You might want to save them later for additional evaluation, when you have a new fine-tuned model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that is the correct path\n",
    "model_path=\"cognitivecomputations/dolphin-2.1-mistral-7b\"\n",
    "# model id will be used to load our conversation template https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L1579\n",
    "model_id=\"mistral-dolphin-sft\"\n",
    "\n",
    "# generate model answer\n",
    "!python gen_model_answer.py --model-id {model_id} --model-path {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Generating all responses can take a while, ~60 minutes or more._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate the responses using the DPO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to where you saved the model during training, remember our current directory is FastChat/\n",
    "model_path=\"/fsx/philipp/doplhin-dpo\"\n",
    "\n",
    "# model id will be used to load our conversation template https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L1579\n",
    "model_id=\"mistral-dolphin-dpo\"\n",
    "\n",
    "# generate model answer\n",
    "!python gen_model_answer.py --model-id {model_id} --model-path {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Generating all responses can take a while, ~120 minutes or more._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate the responses using pair-wise comparison and GPT-4-Turbo as Judge\n",
    "\n",
    "After we have the responses we can evaluate them using the `gen_judgment.py` script. This will pairwise compare all the responses using GPT-4-Turbo and rate which response is better.\n",
    "\n",
    "_Note: We need an `OPENAI_API_KEY` with access to GPT-4 Turbo, running MT-Bench will cost ~1-2$ per model evaluation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai_key=\"sk-xxxx\" # replace with your openai key\n",
    "\n",
    "# Pairwise comparison of the two models using OpenAI's GPT-4 Turbo\n",
    "!OPENAI_API_KEY={open_ai_key} python gen_judgment.py --model-list \"mistral-dolphin-dpo\" \"mistral-dolphin-sft\" --judge-model \"gpt-4-1106-preview\" --mode \"pairwise-all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: This can take ~70 minutes._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot and compare the results\n",
    "\n",
    "After we have the results we can plot them and compare the win-rate of the SFT and DPO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are saved at the following location, make sure its correct\n",
    "res = \"./data/mt_bench/model_judgment/gpt-4-1106-preview_pair.jsonl\"\n",
    "\n",
    "!python show_result.py --input-file {res} --model-list \"mistral-dolphin-dpo\" \"mistral-dolphin-sft\" --judge-model \"gpt-4-1106-preview\" --mode pairwise-all\n",
    "\n",
    "# display plot from image file\n",
    "from PIL import Image\n",
    "i = Image.open(\"win_rate_gpt-4-1106-preview.png\")\n",
    "i.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| model               |   win |   loss |   tie |   win_rate |   loss_rate |   win_rate_adjusted |\n",
    "|:--------------------|------:|-------:|------:|-----------:|------------:|--------------------:|\n",
    "| mistral-dolphin-dpo |    45 |     17 |    98 |    0.28125 |     0.10625 |              0.5875 |\n",
    "| mistral-dolphin-sft |    17 |     45 |    98 |    0.10625 |     0.28125 |              0.4125 |\n",
    "\n",
    "![image](../assets//win_rate_gpt-4-1106-preview.png)\n",
    "\n",
    "By using DPO we were able to achieve a win-rate of 0.5875 compared to 0.4125 with the SFT model. This means by applying DPO we tuned our model to generate responses, which are more aligned with what humans/AI would prefer. This is not optimal yet, but it's a good start. \n",
    "\n",
    "Since the guide is only a starting point, you should consider additional evaluation methods, e.g. human evaluation or instruction-following capabilities. This means we might not have reached the full potential of the model. You should consider training for more epochs and on a larger dataset to improve the model further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clean up the FastChat Repository\n",
    "\n",
    "Since we temporary cloned the FastChat repository we can now clean it up by deleting the directory.\n",
    "\n",
    "_Note: If you want to keep your evaluation results you should save the `model_answer` and `judgment` directory._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../../..\n",
    "\n",
    "# delete the cloned repository\n",
    "!rm -rf FastChat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
