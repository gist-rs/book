{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF in 2024 with DPO & Hugging Face\n",
    "\n",
    "This blog post walks you through how to use DPO to improve open LLMs using Hugging Face [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index) & [datasets](https://huggingface.co/docs/datasets/index) in 2024. \n",
    "\n",
    "Research and experiments suggest that DPO should only be applied after SFT. This means we need an already fine-tuned LLM, which can be aligned with DPO. In this example we will use [cognitivecomputations/dolphin-2.1-mistral-7b](https://huggingface.co/cognitivecomputations/dolphin-2.1-mistral-7b) a fine-tuned Mistral 7B with ChatML template. \n",
    "\n",
    "1. Setup development environment\n",
    "2. Create and prepare the preference dataset\n",
    "3. Align LLM with TRL and the DPOTrainer\n",
    "4. Test LLM (vibe-check)\n",
    "5. Evaluate open LLMs on MT-Bench\n",
    "\n",
    "\n",
    "_Note: This example is designed to be an introduction to DPO and TRL. It is build for a single GPU environment to guide you through the process. For production use, you should consider using a distributed environment. It should be possible to run the example on a single GPU with at least 24GB of memory by reducing the training arguments, with batch size, max seq length and run evaluation after the training._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries and Pytorch, including trl, transformers and datasets. If you haven't heard of trl yet, don't worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers[sentencepiece]==4.37.2\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "Collecting datasets==2.16.1\n",
      "  Using cached datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "Requirement already satisfied: accelerate==0.26.1 in ./.venv/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: evaluate==0.4.1 in ./.venv/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: bitsandbytes==0.42.0 in ./.venv/lib/python3.10/site-packages (0.42.0)\n",
      "Collecting trl==0.7.11\n",
      "  Using cached trl-0.7.11-py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: peft==0.8.2 in ./.venv/lib/python3.10/site-packages (0.8.2)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.10/site-packages (10.2.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (0.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (23.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (2023.12.25)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (4.66.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (3.13.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (0.20.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (0.2.0)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (4.25.3)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (3.4.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (0.6)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (2.2.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (0.3.7)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (3.9.3)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in ./.venv/lib/python3.10/site-packages (from datasets==2.16.1) (2023.10.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from accelerate==0.26.1) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.venv/lib/python3.10/site-packages (from accelerate==0.26.1) (2.2.1)\n",
      "Requirement already satisfied: responses<0.19 in ./.venv/lib/python3.10/site-packages (from evaluate==0.4.1) (0.18.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.10/site-packages (from bitsandbytes==0.42.0) (1.12.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in ./.venv/lib/python3.10/site-packages (from trl==0.7.11) (0.7.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]==4.37.2) (4.10.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.37.2) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.37.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.37.2) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.37.2) (2024.2.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (2.19.3)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (10.3.2.106)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.0.106)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.26.1) (12.4.99)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in ./.venv/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in ./.venv/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./.venv/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11) (1.7.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.26.1) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (0.1.2)\n",
      "Installing collected packages: transformers, datasets, trl\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.37.0\n",
      "    Uninstalling transformers-4.37.0:\n",
      "      Successfully uninstalled transformers-4.37.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.7.4\n",
      "    Uninstalling trl-0.7.4:\n",
      "      Successfully uninstalled trl-0.7.4\n",
      "Successfully installed datasets-2.16.1 transformers-4.37.2 trl-0.7.11\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "# !pip install \"torch==2.1.2\" tensorboard\n",
    "\n",
    "# Install Hugging Face libraries\n",
    "!pip install  --upgrade \\\n",
    "  \"transformers[sentencepiece]==4.37.2\" \\\n",
    "  \"datasets==2.16.1\" \\\n",
    "  \"accelerate==0.26.1\" \\\n",
    "  \"evaluate==0.4.1\" \\\n",
    "  \"bitsandbytes==0.42.0\" \\\n",
    "  \"trl==0.7.11\" \\\n",
    "  \"peft==0.8.2\" \\\n",
    "  \"pillow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using a GPU with Ampere architecture (e.g. NVIDIA A10G or RTX 4090/3090) or newer, you can use Flash Attention. **[FlashAttention](https://github.com/Dao-AILab/flash-attention/tree/main)** can accelerate training time up to 3x.\n",
    "\n",
    "*Note: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of **`MAX_JOBS`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in ./.venv/lib/python3.10/site-packages (1.11.1.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (23.2)\n",
      "Requirement already satisfied: flash-attn in ./.venv/lib/python3.10/site-packages (2.5.6)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from flash-attn) (23.2)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (from flash-attn) (2.2.1)\n",
      "Requirement already satisfied: einops in ./.venv/lib/python3.10/site-packages (from flash-attn) (0.7.0)\n",
      "Requirement already satisfied: ninja in ./.venv/lib/python3.10/site-packages (from flash-attn) (1.11.1.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.0.106)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (10.3.2.106)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (4.10.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (11.4.5.107)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (8.9.2.26)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (2023.10.0)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "# install flash-attn\n",
    "!pip install ninja packaging\n",
    "!MAX_JOBS=1 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Installing flash attention can take quite a bit of time (10-45 minutes)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [Hugging Face Hub](https://huggingface.co/models) as a remote model storage and automatically push our model, logs and information to the Hub during training. You must register on the [Hugging Face](https://huggingface.co/join) for this. After you have an account, we will use the `login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "export HF_TOKEN=foobar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "\n",
    "# login(\n",
    "#   token=\"\", # ADD YOUR TOKEN HERE\n",
    "#   add_to_git_credential=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and prepare the dataset\n",
    "\n",
    "Improving the helpfulness or quality of LLMs through Aligning methods like DPO doesn’t come for free. Compared to traditional supervised fine-tuning (SFT) alignment methods require preference data. Preference data is crucial as it serves as a proxy against which the model's outputs are evaluated and aligned. A typical DPO dataset includes a triplet out of prompt, chosen, and rejected response. There are several ways to create such a dataset, including:\n",
    "\n",
    "- Using existing open-source datasets, e.g., [SHP](https://huggingface.co/datasets/stanfordnlp/SHP)\n",
    "- Using LLMs to create synthetic preferences, e.g., [Ultrafeedback](https://www.notion.so/9de9ac96f0f94aa5aed96361a26e8bf0?pvs=21)\n",
    "- Using Humans to create datasets, e.g., [HH](https://www.notion.so/SageMaker-bi-weekly-sync-0be2e6ba876a4599b4c0da2681dfb78f?pvs=21)\n",
    "- Using a combination of the above methods, e.g., [Orca DPO](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n",
    "\n",
    "Each method has advantages and disadvantages and depends on the budget, time, and quality requirements. \n",
    "\n",
    "*It's important to recognize that preference datasets can inherently reflect the biases of the human/AI they are based on. To ensure broader applicability and fairness, it's crucial to incorporate a diverse range of feedback in creating these datasets.*\n",
    "\n",
    "In our example, we will use the [argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned?row=0) dataset. The best DPO dataset represents the real-world preferences of your users or customers. If you don’t have collected preferences yet, start with your existing SFT data and use different sizes/quality LLMs to generate feedback. This method was used to create the Orca DPO dataset, where GPT-4 was used for the accepted responses and Llama 70B Chat for the rejected responses. A DPO dataset will have the following format\n",
    "\n",
    "```json\n",
    "{\"chosen\": \"<prompt + good response>\", \"rejected\": \"<prompt + worse response>\" }\n",
    "{\"chosen\": \"<prompt + good response>\", \"rejected\": \"<prompt + worse response>\" }\n",
    "{\"chosen\": \"<prompt + good response>\", \"rejected\": \"<prompt + worse response>\" }\n",
    "```\n",
    "\n",
    "The `<pompt + good response>` and `<prompt + worse response>` are representend in the `conversational` format as: \n",
    "\n",
    "```json\n",
    "[{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]\n",
    "```\n",
    "\n",
    "_**Note:** If the dataset includes multiple turns you need to make sure that only the last turn between chosen and rejected is different. If not, you must reduce the conversation until only the last assistant turn is different._\n",
    "\n",
    "\n",
    "The DPOTrainer expects the inputs as triples of (prompt, chosen, rejected), where `chosen` and `rejected` are the final turn of a dialogue and the `prompt` is N-1 turns. Those inputs also need to be already formated with the tempalte of the model, e.g. `<|im_start|>user\\nINSTRUCTION\\n<|im_end|>\\n<|im_start|>assistant\\n...`.\n",
    "\n",
    "In our example we are going to load our open-source dataset using the 🤗 Datasets library and then convert it into the correct format. The  [argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned?row=0) already comes with the DPO format (chosen/rejected). This means we can create our triplet and templetite it usng a `tokenizer` and the `apply_chat_template` methoh. We are randomly downsampling the dataset to 11,000 training samples and 2,750 test samples\n",
    "\n",
    "_Note: This step can be different for your use case. For example, if you might need to create the `conversational` format and concate the prompt and chosen/rejected response, e.g. `Human:\\n ... Assistant:\\n`._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katopz/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load Tokenizer from the hub\n",
    "model_id = \"cognitivecomputations/dolphin-2.1-mistral-7b\" # replace with your model id\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"argilla/ultrafeedback-binarized-preferences-cleaned\", split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'prompt', 'chosen', 'chosen-rating', 'chosen-model', 'rejected', 'rejected-rating', 'rejected-model'],\n",
       "    num_rows: 240\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_extract_assistant_messages(messages, index=-1):\n",
    "  \"\"\"Recursively extract the last assistant messages from the end of the conversation.\"\"\"\n",
    "  if messages[index][\"role\"] == \"assistant\":\n",
    "    return [messages[index]]\n",
    "  else:\n",
    "    return rec_extract_assistant_messages(messages, index-1)\n",
    "    \n",
    "# System message used if there is no system message at the beginning of the conversation\n",
    "# Can be repelaced and modified as needed\n",
    "DEFAULT_SYSTEM_MESSAGE = \"You are Dolphin, a helpful AI assistant.\"\n",
    "\n",
    "def create_triplets(example, tokenizer, default_system_message=DEFAULT_SYSTEM_MESSAGE):\n",
    "  \"\"\"Create the triplets (prompt, chosen, rejected)\"\"\"\n",
    "  # Extract the N-1 turns to form the prompt\n",
    "  # Prepend a system message if the first message is not a system message\n",
    "  prompt_messages = example[\"chosen\"][:-1]\n",
    "  if example[\"chosen\"][0][\"role\"] != \"system\":\n",
    "      prompt_messages.insert(0, {\"role\": \"system\", \"content\": default_system_message})\n",
    "  # Now we extract the final assistant turn to define chosen/rejected responses \n",
    "  chosen_messages = rec_extract_assistant_messages(example[\"chosen\"])\n",
    "  rejected_messages = rec_extract_assistant_messages(example[\"rejected\"])\n",
    "  \n",
    "  # apply template to the messages and return the triplets\n",
    "  return {\n",
    "    \"prompt\": tokenizer.apply_chat_template(prompt_messages, tokenize=False),\n",
    "    \"chosen\": tokenizer.apply_chat_template(chosen_messages, tokenize=False),\n",
    "    \"rejected\": tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 240/240 [00:00<00:00, 2025.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(create_triplets, remove_columns=dataset.features, fn_kwargs={\"tokenizer\": tokenizer})  \n",
    "# split dataset into 200 training samples and 40 test samples\n",
    "dataset = dataset.train_test_split(test_size=40/240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Dolphin, a helpful AI a\n",
      "<|im_start|>assistant\n",
      "{\n",
      "  \"program\": {\n",
      "    \"title\"\n",
      "<|im_start|>assistant\n",
      " well 1 Spl a” Donnelly (Aug\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 300.86ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 674.54ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128492"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print sample cut of \n",
    "print(dataset[\"train\"][0][\"prompt\"][:50])\n",
    "print(dataset[\"train\"][0][\"chosen\"][:50])\n",
    "print(dataset[\"train\"][0][\"rejected\"][:50])\n",
    "\n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Align LLM with TRL and the DPOTrainer\n",
    "\n",
    "TRL supports the DPO through a dedicated [DPOTrainer](https://huggingface.co/docs/trl/dpo_trainer) for alinging LLMs from preference data, as described in [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290). The `DPOTrainer` is a subclass of the `Trainer` from the `transformers` library and supports all the same features, including logging, evaluation, and checkpointing. \n",
    "\n",
    "One big difference to SFT is that for DPO we need an additional Reference Model, which is used for KL-Divergence to help stabilize the training. The Reference Model is normally the same model as the one we are training, but frozen. This means for DPO you need additional memory and compute resources. To keep our example efficient we will use PEFT and adatpers. We load your fine-tuned and then add a new trainable adapters. This means that we will only tune adapters and not the whole model using DPO. The origian model will be then used as reference model itself. If you want to train all parameter with DPO you need to provide a `model` and `reference_model, but this requires more memory and compute resources.\n",
    "\n",
    "Lets start by loading our saved datasets from disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 200 examples [00:00, 40446.52 examples/s]\n",
      "Generating train split: 40 examples [00:00, 20184.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load jsonl data from disk\n",
    "train_dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train [cognitivecomputations/dolphin-2.1-mistral-7b](https://huggingface.co/cognitivecomputations/dolphin-2.1-mistral-7b). Dolphin is a fine-tuned Mistral 7B with ChatML template support system messages. You can easily swap out the model for another model, e.g. [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) or [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) models, TII [Falcon](https://huggingface.co/tiiuae/falcon-40b), or any other LLMs by changing our `model_id` variable.\n",
    "\n",
    "_Note: Be aware the bigger the model the more memory it will require. In our example we will use the 7B version, which can be tuned on 24GB GPUs. If you have a smaller GPU._\n",
    "\n",
    "The first step is to load the model in int-4 using `bitsandbytes` and then add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.92s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"cognitivecomputations/dolphin-2.1-mistral-7b\" # replace with your model id\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=False, \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' # to prevent errors with FA\n",
    "tokenizer.truncation_side = 'left' # to prevent cutting off last generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the `SFTTrainer` the DPOTrainer has two parameter related to dataset sizing with `max_prompt_length` and `max_length`. The `max_prompt_length` is the maximum length of the prompt and the `max_length` is the maximum length of the prompt + chosen or rejected response. Those are used for tokenization, padding and trunctation. This means if we set those wrongly our data will be potentially cut off, but if we set them too high we will waste memory and time. \n",
    "\n",
    "The [Alignment Handbook](https://github.com/huggingface/alignment-handbook) when with the `max_prompt_length` of 512 and `max_length` of 1024 combining it with the truncation side left (90% of data samples where in that range). Truncation side left  means the beginning will be removed so we keep the important assistant response. In our example we want to cover the ~97% percentile and filter out longer samples, rather than truncating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 200/200 [00:00<00:00, 3910.34 examples/s]\n",
      "Filter: 100%|██████████| 40/40 [00:00<00:00, 1771.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset): 190\n",
      "len(eval_dataset): 39\n",
      "p95 prompt length: 726\n",
      "p95 prompt + chosen length: 1164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### COMMENT IN TO RECALCULATE MAX LENGTHS ####\n",
    "from numpy import percentile\n",
    "\n",
    "# lets find the p95 length of the prompt \n",
    "prompt_length = int(percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"prompt\"]], 95))\n",
    "max_seq_length_chosen = int(percentile([len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
    "max_seq_length_rejected = int(percentile([len(tokenizer(x[\"prompt\"] + x[\"rejected\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
    "max_seq_length = max(max_seq_length_chosen, max_seq_length_rejected)\n",
    "\n",
    "# filter datasets to remove samples that are too long\n",
    "train_dataset = train_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
    "eval_dataset = eval_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
    "print(f\"len(train_dataset): {len(train_dataset)}\")\n",
    "print(f\"len(eval_dataset): {len(eval_dataset)}\")\n",
    "\n",
    "# Up the lengths to next multiple of 2, why 2? Don't know\n",
    "prompt_length = ((prompt_length + 1) // 2) * 2\n",
    "max_seq_length = ((max_seq_length + 1) // 2) * 2\n",
    "print(f\"p95 prompt length: {prompt_length}\")\n",
    "print(f\"p95 prompt + chosen length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_length = 1024\n",
    "max_seq_length = 1512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: You could reduce the `max_seq_length` to `1512` this would lead to a memory reduction and then increase the batch_size._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DPOTrainer` supports a native integration with `peft`, which makes it super easy to efficiently align LLMs using, e.g. QLoRA. We only need to create our `LoraConfig` and provide it to the trainer. Our `LoraConfig` parameters are the same as for the SFT example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the LoraConfig class from the peft module\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Creating a LoRA configuration based on the QLoRA paper & Sebastian Raschka's experiment\n",
    "peft_config = LoraConfig(\n",
    "    # Parameter for LoRA's alpha value,\n",
    "    # controls the importance the model gives to the current input\n",
    "    # compared to historical inputs.\n",
    "    lora_alpha=16,\n",
    "    \n",
    "    # Dropout probability, a regularization technique to prevent overfitting\n",
    "    lora_dropout=0.05,\n",
    "    \n",
    "    # Parameter 'r' for LoRA,\n",
    "    # determines the number of historical context embeddings used in current predictions\n",
    "    r=64, # 64 previous time steps.\n",
    "    \n",
    "    # Bias parameter, specifying whether to use bias terms in LoRA\n",
    "    bias=\"none\",\n",
    "\n",
    "    # Target modules for LoRA,\n",
    "    # indicating which layers should use the LoRA mechanism\n",
    "    target_modules=\"all-linear\",\n",
    "    \n",
    "    # Task type, specifying the type of task for which LoRA is being configured\n",
    "    # (here, a causal language modeling task: the model predicts the next word in\n",
    "    # a sentence based only on the words that came before it)\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start our training we need to define the hyperparameters (`TrainingArguments`) & DPO parameters.\n",
    "\n",
    "Based on the [Alignment Handbook](https://github.com/huggingface/alignment-handbook) we know that we need to use a ~10-100x smaller learning rate for DPO compared to SFT. In our example we reduce the learning rate from 2e-4 (SFT) to 5e-5 (DPO) or 40x smaller.  \n",
    "\n",
    "Another important parameter is the `beta` parameter, which is used to control the strength of the alignment. The bigger the `beta` is typically something in the range of 0.1 to 0.5. A higher beta means less divergence from the initial reference model or the text generations are very similar in terms of their probability distributions. In terms of training length, we go with `1` epoch, which is a good starting point. There is no rule of thumb for the number of epochs, it is also related to the number of epochs used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"doplhin-dpo\",               # directory to save and repository id\n",
    "    num_train_epochs=1,                     # number of training epochs\n",
    "    per_device_train_batch_size=6,         # batch size per device during training\n",
    "    per_device_eval_batch_size=2,           # batch size for evaluation\n",
    "    gradient_accumulation_steps=1,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    learning_rate=5e-5,                     # 10x higher LR than QLoRA paper\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.1,                       # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"cosine\",             # use cosine learning rate scheduler\n",
    "    logging_steps=25,                       # log every 25 steps\n",
    "    save_steps=500,                         # when to save checkpoint\n",
    "    save_total_limit=2,                     # limit the total amount of checkpoints\n",
    "    evaluation_strategy=\"steps\",            # evaluate every 1000 steps\n",
    "    eval_steps=700,                         # when to evaluate\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    push_to_hub=False,                      # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")\n",
    "\n",
    "dpo_args = {\n",
    "    \"beta\": 0.1,                            # The beta factor in DPO loss. Higher beta means less divergence\n",
    "    \"loss_type\": \"sigmoid\"                  # The loss type for DPO.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have every building block we need to create our `DPOTrainer` to start then training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katopz/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:328: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 190/190 [00:00<00:00, 1024.09 examples/s]\n",
      "Map: 100%|██████████| 39/39 [00:00<00:00, 900.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import DPOTrainer\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None, # set to none since we use peft\n",
    "    peft_config=peft_config,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_seq_length,\n",
    "    max_prompt_length=prompt_length,\n",
    "    beta=dpo_args[\"beta\"],\n",
    "    loss_type=dpo_args[\"loss_type\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training our model by calling the `train()` method on our `Trainer` instance. This will start the training loop and train our model for 2 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model.\n",
    "\n",
    "_Note: During the training we want to minimize loss and grow reward/margins metrics. Keep an eye on the reward/margins metrics, if they are not growing you might need to increase the `beta` parameter or adjust the `learning_rate`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katopz/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "/home/katopz/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 09:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32, training_loss=0.7861262708902359, metrics={'train_runtime': 608.4546, 'train_samples_per_second': 0.312, 'train_steps_per_second': 0.053, 'total_flos': 0.0, 'train_loss': 0.7861262708902359, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model at the end of training\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trl.trainer.dpo_trainer.DPOTrainer at 0x7f6279d85600>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training with Flash Attention for 1 epochs with a dataset of ~10k samples took ~01:30:00 on 1x H100 GPU. You should be able to run the training on a `g5.2xlarge` instance by reducing the batch_size (est. to 1) and maybe the max_seq_length (est. to 1512)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Optional: Merge LoRA adapter in to the original model_\n",
    "\n",
    "When using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the `merge_and_unload` method and then save the model with the `save_pretrained` method. This will save a default model, which can be used for inference.\n",
    "\n",
    "_Note: You might require > 30GB CPU Memory._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:39<00:39, 39.57s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")  \n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test LLM (vibe-check)\n",
    "\n",
    "After the training is done we want to test and evaluate or model. Evaluating Generative AI models in an open-ended way is not a trivial since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out [Evaluate LLMs and RAG a practical example using Langchain and Hugging Face](https://www.philschmid.de/evaluate-llm) blog post. Especially, when using RLHF techniques like DPO, it's important to \"vibe-check\" the model. \n",
    "\n",
    "This means we want to manually check if the responses are more aligned with what our users or customers want. This could mean that we need to check if the responses are more helpful, more accurate, more engaging, or more informative as before. A good test here is if you have data from your SFT or previous LLMs, you can compare the outputs and see if the new model is better.\n",
    "\n",
    "In our case we just check a few examples and see if the model generates helpful responses using unseen prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katopz/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.69s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline \n",
    "from pprint import pprint\n",
    "\n",
    "# Path to saved peft adapter model\n",
    "# peft_model_id = args.output_dir # or\n",
    "peft_model_id = \"./doplhin-dpo\" \n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "finetuned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "# load into pipeline\n",
    "finetuned_pipe = pipeline(\"text-generation\", model=finetuned_model, tokenizer=finetuned_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(pipe, tokenizer, prompt, temperature=1.0, top_k=50, top_p=0.9):\n",
    "  # messages = pipe.tokenizer.apply_chat_template([{\"role\":\"user\", \"content\": prompt}], tokenize=False)\n",
    "  outputs = pipe(prompt, max_new_tokens=2048, do_sample=True, temperature=temperature, top_k=top_k, top_p=top_p, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)\n",
    "  \n",
    "  print(f\"**Prompt**:\\n{prompt}\\n\")\n",
    "  print(f\"**Generated Answer**:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomely select prompts from the [teknium/OpenHermes-2.5](https://huggingface.co/datasets/teknium/OpenHermes-2.5) dataset and a Hugging Face special."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "  \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
    "  \"It's Bengay for muscle relief, a combination of methyl salicylate, menthol, and what other active ingredient commonly found in aspirin?\",\n",
    "  \"How can i get rid of llamas in my backyard?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets iterate over the prompts and generate a response using the `generate` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Prompt**:\n",
      "A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\n",
      "\n",
      "**Generated Answer**:\n",
      "Let's calculate the perimeter of the rectangle:\n",
      "\n",
      "Perimeter = 2(length) + 2(width)\n",
      "Perimeter = 2(25) + 2(15)\n",
      "Perimeter = 50 + 30\n",
      "Perimeter = 80 feet\n",
      "\n",
      "So, you will need 80 feet of fencing to enclose the garden.\n",
      "\n",
      "Now let's consider an irregular shaped garden with the following measurements:\n",
      "\n",
      "- One side measures 10 feet\n",
      "- Two sides measure 15 feet each\n",
      "- Two sides measure 5 feet each\n",
      "\n",
      "To find the perimeter of this irregularly shaped garden, we add up the lengths of all the sides:\n",
      "\n",
      "Perimeter = 10 + 15 + 15 + 5 + 5\n",
      "Perimeter = 50 feet\n",
      "\n",
      "In this case, you will also need 50 feet of fencing to enclose the garden.\n",
      "\n",
      "In both examples, the total amount of fencing required is the same (80 feet). However, the methods used to calculate the perimeter differ depending on the shape of the garden.\n",
      "\n",
      "So, the formula for finding the perimeter of a rectangle is 2(length) + 2(width), while for irregular shapes, you simply add up the lengths of all the sides.\n",
      "**Prompt**:\n",
      "It's Bengay for muscle relief, a combination of methyl salicylate, menthol, and what other active ingredient commonly found in aspirin?\n",
      "\n",
      "**Generated Answer**:\n",
      "A) Caffeine B) Eucalyptus Oil C) Camphor D) Peppermint Oil\n",
      "\n",
      "Answer: C) Camphor\n",
      "\n",
      "6. What common medication can help treat diarrhea by helping to slow the movement of food through the intestines?\n",
      "\n",
      "A) Laxatives B) Antacids C) Anti-diarrheal medicines D) Pain relievers\n",
      "\n",
      "Answer: C) Anti-diarrheal medicines\n",
      "\n",
      "7. This active ingredient, when applied to the skin in an ointment or cream, can help relieve pain associated with muscle strains and sprains.\n",
      "\n",
      "A) Aspirin B) Alcohol C) Ibuprofen D) Menthol\n",
      "\n",
      "Answer: C) Ibuprofen\n",
      "\n",
      "8. Which of these is commonly found in over-the-counter medications that help treat allergy symptoms such as sneezing, runny nose, and itchy, watery eyes?\n",
      "\n",
      "A) Caffeine B) Quinine C) Phenylephrine D) Dextromethorphan\n",
      "\n",
      "Answer: C) Phenylephrine\n",
      "\n",
      "9. This common medication is often used to treat headaches and migraines, helping to reduce inflammation and relieve pain.\n",
      "\n",
      "A) Antacids B) Antihistamines C) Analgesics D) Antivirals\n",
      "\n",
      "Answer: C) Analgesics\n",
      "\n",
      "10. If you're feeling nauseated and might be coming down with the flu, which of these medications can help to relieve your symptoms?\n",
      "\n",
      "A) Anti-diarrheal medicines B) Antacids C) Antiemetics D) Pain relievers\n",
      "\n",
      "Answer: C) Antiemetics\n",
      "\n",
      "It's clear that we live in a world where a diverse range of medications is available to help us with all kinds of ailments. Next time you need to treat a headache or soothe some muscle pain, you'll be better equipped to identify the right medication for the job. And of course, always be sure to follow the directions on the package and consult with a healthcare professional if necessary.\n",
      "\n",
      "So, how did you do on our quiz? Remember, even if you didn't know all the answers, it's always a good idea to learn more about the medications you're taking and their potential side effects. Knowledge is power, and when it comes to your health, being well-informed can make a world of difference.\n",
      "**Prompt**:\n",
      "How can i get rid of llamas in my backyard?\n",
      "\n",
      "**Generated Answer**:\n",
      "If you've discovered wild llamas wandering into your backyard, try to determine if they belong to someone nearby. Contact your local animal control, as well as your neighbors, to see if the animals are stray or if they've escaped from a nearby property. If the llamas do belong to someone, you may be able to work with the owner to find a solution for keeping them off your property.\n",
      "\n",
      "If the llamas are truly stray or wild, you can try to discourage them from entering your property by installing fencing or other barriers. You may also want to consider planting bushes or trees that llamas find unpalatable to deter them from coming into your yard.\n",
      "\n",
      "Finally, if all else fails, contact your local wildlife agency for advice on how to handle the situation. They may be able to provide you with more specific solutions based on the type of wildlife you're dealing with and the laws in your area.\n",
      "\n",
      "## How do I stop llamas from jumping over fences?\n",
      "\n",
      "If you're dealing with llamas that are jumping over fences, there are a few steps you can take to discourage this behavior:\n",
      "\n",
      "1. Ensure your fences are tall enough: Llamas can be quite agile, so it's important that your fencing is tall enough to discourage them from attempting to jump over it. A fence that's at least 5 feet tall should be sufficient in most cases.\n",
      "\n",
      "2. Use electric fencing: Electric fencing can be an effective deterrent for llamas, as they generally don't like the sensation of an electric shock. This can help train them to stay away from your fence.\n",
      "\n",
      "3. Add barriers: You can add additional barriers, such as chicken wire or wire mesh, to the top of your fence to make it more difficult for the llamas to get a good grip and jump over.\n",
      "\n",
      "4. Keep your fence in good condition: Repair any gaps or holes in your fence, as these can provide llamas with an easy way to jump over.\n",
      "\n",
      "5. Train your llamas: If you have a llama that keeps jumping over the fence, it's possible that they're doing so out of curiosity or boredom. Training them to stay within a designated area can help curb this behavior.\n",
      "\n",
      "6. Consider hiring a professional: If you're having trouble keeping your llamas contained, you may want to consider hiring a professional who specializes in llama containment or fence repair.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Llamas are fascinating creatures and can make excellent pets or companions for those who are prepared to care for them properly. However, they can also be quite mischievous and may try to jump over fences to explore new areas. By following the tips and tricks outlined above, you can keep your llamas safely contained within your property and avoid any potential problems.\n",
      "\n",
      "Remember to always provide your llamas with a comfortable and secure living environment, and to regularly check and maintain your fences to ensure they remain in good condition. With the right care and attention, your llamas will stay happily contained and you can enjoy all the benefits of having these lovable animals as part of your family.\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "  generate_answer(pipe=finetuned_pipe, tokenizer=finetuned_tokenizer, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.38s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "base_model_id = \"cognitivecomputations/dolphin-2.1-mistral-7b\"\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "  base_model_id,\n",
    "  device_map=\"cuda:0\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "# load into pipeline\n",
    "base_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=base_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|im_start|>assistant\\n'\n",
      " 'There are many great projects that you can work on as a Python beginner. '\n",
      " 'Here are a few ideas:\\n'\n",
      " '\\n'\n",
      " '* **A calculator:** This is a great way to learn the basics of Python syntax '\n",
      " 'and data types. You can start by creating a simple calculator that can add, '\n",
      " 'subtract, multiply, and divide two numbers. Then, you can add more features, '\n",
      " 'such as the ability to perform more complex operations, like exponentiation '\n",
      " 'and logarithms.\\n'\n",
      " '* **A to-do list:** This is another great way to practice your Python '\n",
      " 'skills. You can use a to-do list to keep track of your tasks and deadlines. '\n",
      " 'You can also use it to track your progress on larger projects.\\n'\n",
      " '* **A simple game:** This is a fun and challenging way to learn more about '\n",
      " 'Python. You can create a simple game, such as a tic-tac-toe or hangman game. '\n",
      " 'As you become more experienced, you can create more complex games, such as a '\n",
      " 'card game or a puzzle game.\\n'\n",
      " '* **A data analysis project:** This is a great way to learn how to use '\n",
      " 'Python to analyze data. You can use a Python library, such as pandas, to '\n",
      " 'read data from a file and then use it to create charts and graphs. You can '\n",
      " 'also use Python to perform statistical analysis on your data.\\n'\n",
      " \"* **A web application:** This is a more advanced project, but it's a great \"\n",
      " 'way to learn how to use Python to create a web application. You can use a '\n",
      " 'framework, such as Django or Flask, to make the process easier.\\n'\n",
      " '\\n'\n",
      " 'These are just a few ideas to get you started. There are many other great '\n",
      " 'projects that you can work on, so be creative and have fun!<|im_end|>\\n')\n"
     ]
    }
   ],
   "source": [
    "ans = {\"prompt\":\"<|im_start|>system\\nYou are Dolphin, a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nwhat project can I work on as a Python beginner?<|im_end|>\\n\",\"chosen\":\"<|im_start|>assistant\\nThere are many great projects that you can work on as a Python beginner. Here are a few ideas:\\n\\n* **A calculator:** This is a great way to learn the basics of Python syntax and data types. You can start by creating a simple calculator that can add, subtract, multiply, and divide two numbers. Then, you can add more features, such as the ability to perform more complex operations, like exponentiation and logarithms.\\n* **A to-do list:** This is another great way to practice your Python skills. You can use a to-do list to keep track of your tasks and deadlines. You can also use it to track your progress on larger projects.\\n* **A simple game:** This is a fun and challenging way to learn more about Python. You can create a simple game, such as a tic-tac-toe or hangman game. As you become more experienced, you can create more complex games, such as a card game or a puzzle game.\\n* **A data analysis project:** This is a great way to learn how to use Python to analyze data. You can use a Python library, such as pandas, to read data from a file and then use it to create charts and graphs. You can also use Python to perform statistical analysis on your data.\\n* **A web application:** This is a more advanced project, but it's a great way to learn how to use Python to create a web application. You can use a framework, such as Django or Flask, to make the process easier.\\n\\nThese are just a few ideas to get you started. There are many other great projects that you can work on, so be creative and have fun!<|im_end|>\\n\",\"rejected\":\"<|im_start|>assistant\\nThere are many projects you can work on as a Python beginner. Here are some ideas:\\n\\n1. Build a simple chat application: You can create a chat application that allows users to communicate with each other. This project will help you learn how to use TCP\\/IP networking, sockets, and how to handle user input.\\n\\n2. Create a program that generates random numbers: With this project, you will learn how to use random functions in Python and how to generate different types of random numbers.\\n\\n3. Develop a game: You can create a simple game, such as a tic-tac-toe game or a hangman game. This project will help you learn how to use loops, conditionals, and event handling.\\n\\n4. Build a web scraping tool: You can create a tool that scrapes data from a website, such as a stock price tracker or a weather API. This project will help you learn how to use Python libraries like BeautifulSoup and requests.\\n\\n5. Automate a task with Python: You can write a script that automates a repetitive task, such as downloading files from a website or extracting data from a PDF. This project will help you learn how to use Python scripts to automate tasks.<|im_end|>\\n\"}\n",
    "\n",
    "pprint(ans[\"chosen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Prompt**:\n",
      "what project can I work on as a Python beginner?\n",
      "\n",
      "**Generated Answer**:\n",
      "Beginners can find it challenging to choose a project for themselves. There are many python projects for beginners:\n",
      "\n",
      "1. Pizza Ordering System\n",
      "2. Calculator\n",
      "3. Password Generator\n",
      "4. Todo App\n",
      "5. Image Processor\n",
      "6. Dice Roller\n",
      "7. Library Management System\n",
      "8. Instagram Auto Liker\n",
      "9. Video Game\n",
      "10. Tic-Tac-Toe Game\n",
      "11. Quiz App\n",
      "12. Stock Price Analyzer\n",
      "13. E-commerce Website\n",
      "14. Twitter Bot\n",
      "15. Weather App\n",
      "16. Calendar App\n",
      "17. Chatbot\n",
      "18. Budget Planner\n",
      "19. Music Player\n",
      "20. To-Do List App\n",
      "21. Movie Recommendation System\n",
      "22. Blog Website\n",
      "23. Recipe Management App\n",
      "24. Simple CRM System\n",
      "25. Polling App\n",
      "\n",
      "For more projects, you can check here\n",
      "\n",
      "How to get started as a Python developer?\n",
      "\n",
      "1. Learn the basics: Start with learning the basics of Python such as data types, loops, conditions, functions, etc.\n",
      "\n",
      "2. Install Python: Download and install Python on your system.\n",
      "\n",
      "3. Learn from online resources: There are many online resources available for learning Python like Python official documentation, websites like python.org, and YouTube channels like Corey Schafer and Programming with Mosh.\n",
      "\n",
      "4. Take online courses: You can also take online courses on platforms like Coursera, Udacity, and edX.\n",
      "\n",
      "5. Practice coding: To improve your coding skills, solve Python challenges on websites like LeetCode, HackerRank, and CodeChef.\n",
      "\n",
      "6. Work on Python projects: Start working on small projects to get hands-on experience in Python.\n",
      "\n",
      "7. Build a portfolio: Create a portfolio showcasing your skills and projects.\n",
      "\n",
      "8. Network with others: Join Python communities, forums, and social media groups to network with other developers and learn from them.\n",
      "\n",
      "9. Keep learning: Stay updated with the latest Python libraries, frameworks, and tools.\n",
      "\n",
      "10. Seek a job or internship: Look for job openings or internships related to Python development to gain practical experience.\n"
     ]
    }
   ],
   "source": [
    "generate_answer(pipe=base_pipe, tokenizer=base_tokenizer, prompt=\"what project can I work on as a Python beginner?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Prompt**:\n",
      "what project can I work on as a Python beginner?\n",
      "\n",
      "**Generated Answer**:\n",
      "1. A Simple Text-Based Video Game.\n",
      "2. Calculator.\n",
      "3. Image Processing.\n",
      "4. Build a Weather Dashboard.\n",
      "5. A Simple CRUD App.\n",
      "6. A Web Scraper.\n",
      "7. Python E-commerce Website.\n",
      "8. File Management.\n",
      "\n",
      "## Can I learn Python in a month?\n",
      "\n",
      "If you have the workable knowledge of a programming language already and you dedicate yourself to studying Python for a month, you can certainly become proficient in it. That being said, learning Python in a month is feasible, but you will likely need to study for a couple of hours each day.\n",
      "\n",
      "## What is Python most commonly used for?\n",
      "\n",
      "Python is a general-purpose coding language—which means that it can be used for many different types of software development and problem solving. It’s especially popular within the following frameworks: Web development (server-side) Software development.\n",
      "\n",
      "## Can I learn Python without knowing any programming language?\n",
      "\n",
      "You can start learning Python without knowing any other programming language. However, many people find it helpful to already know at least one programming language, because this makes it easier to understand the concepts and syntax used in Python.\n",
      "\n",
      "## Can I learn Python in a week?\n",
      "\n",
      "Python is a big language with tons of features, so it’s impossible to become an expert in just one week. However, you can definitely become proficient and start writing simple Python programs in a week. If you have experience with other programming languages, this might take even less time.\n",
      "\n",
      "## Can I learn Python in 3 months?\n",
      "\n",
      "You could learn Python in a matter of months, as long as you have the dedication and time to devote to studying. If you’re keen to learn the basics, and want to spend around 5 hours a week studying, you could realistically master the language in three months.\n",
      "\n",
      "## What are the easiest things to make with Python?\n",
      "\n",
      "1. Tic-tac-toe. Tic-tac-toe, or noughts and crosses as it’s known in some parts of the world, is a great introductory game to develop in Python.\n",
      "2. A simple text-based adventure game.\n",
      "3. A calculator.\n",
      "4. A simple chatbot.\n",
      "5. A basic web crawler.\n",
      "6. An RSS reader.\n",
      "7. A Python image viewer.\n",
      "8. A simple cryptocurrency price tracker.\n",
      "\n",
      "## Can Python replace Java?\n",
      "\n",
      "Python can replace Java and JavaScript for back-end development. It’s also useful for other tasks, such as data analysis, artificial intelligence, and deep learning. Because of its versatility and efficiency, more companies are using Python to replace Java and Java Script in web development.\n",
      "\n",
      "## How can I practice Python every day?\n",
      "\n",
      "1. Codecademy. This is one of the best places to learn and practice coding.\n",
      "2. FreeCodeCamp.\n",
      "3. Hackerrank.\n",
      "4. Leetcode.\n",
      "5. Edabit.\n",
      "6. GitHub.\n",
      "7. Kaggle.\n",
      "8. Practice on Your Own Projects.\n",
      "\n",
      "## Is Python enough to get a job?\n",
      "\n",
      "Python might not be the only skill set you’ll need to get a job, but it is an extremely valuable skill to acquire—one that will increase your employability significantly. That said, there are plenty of Python jobs out there, so acquiring this skill could be your foot in the door for a new career.\n",
      "\n",
      "## Is Python worth learning for beginners?\n",
      "\n",
      "Python is a great and easy programming language for beginners to learn. It’s one of the most easy to read and easy to use, so even someone without any programming experience can learn it. Python is used for lots of different tasks including web development, data analysis, and artificial intelligence.\n",
      "\n",
      "## Can I learn Python from scratch?\n",
      "\n",
      "The good news is that you can learn Python from scratch even if you have no prior programming knowledge. To learn Python, you should start by getting a good understanding of the basics of the language and programming in general. Then, you can move on to learning specific skills and technologies.\n",
      "\n",
      "## Can I learn Python and get a job?\n",
      "\n",
      "Python might not be the only skill set you’ll need to get a job, but it is an extremely valuable skill to acquire—one that will increase your employability significantly. That said, there are plenty of Python jobs out there, so acquiring this skill could be your foot in the door for a new career.\n",
      "\n",
      "## Is Python enough to get a job in Google?\n",
      "\n",
      "While Google prefers candidates with a degree in Computer Science, you can still get a job at Google with a degree in any field. The main requirements for getting a job at Google include strong coding skills, knowledge of programming languages like Python and Java, and experience in software development.\n",
      "\n",
      "## Can I learn Python for free?\n",
      "\n",
      "There are plenty of resources online to learn Python for free. Coursera, for example, offers a range of courses in programming languages, including Python. EdX and FutureLearn also have Python courses, while Codecademy and Udemy have Python courses you can do for free.\n",
      "\n",
      "## Is Python harder than JavaScript?\n",
      "\n",
      "Generally, JavaScript is more complex than Python because of its object-based nature and single-threaded architecture. Python is a programming language that is widely used for a myriad of use cases, including web development, data analysis, and artificial intelligence.\n",
      "\n",
      "## Is Python enough for web development?\n",
      "\n",
      "In terms of web development, Python is enough for many different types of projects. However, for certain projects, you might need to complement it with other technologies. Python is particularly good for back-end web development and can be used to create powerful, scalable web applications.\n",
      "\n",
      "## What is the easiest programming language to learn?\n",
      "\n",
      "1. Python. Python is one of the easiest programming languages to learn for beginners.\n",
      "2. Ruby. Ruby is another easy language, especially for beginners looking to learn coding.\n",
      "3. Java.\n",
      "4. JavaScript.\n",
      "5. C.\n",
      "6. C++\n",
      "7. C#\n",
      "8. PHP.\n",
      "\n",
      "## How do I practice Python everyday?\n",
      "\n",
      "1. Codecademy. This is one of the best places to learn and practice coding.\n",
      "2. FreeCodeCamp.\n",
      "3. Hackerrank.\n",
      "4. Leetcode.\n",
      "5. Edabit.\n",
      "6. GitHub.\n",
      "7. Kaggle.\n",
      "8. Practice on Your Own Projects.\n",
      "\n",
      "## Can I learn Python on my own?\n",
      "\n",
      "If you want to learn Python on your own, there are plenty of resources available to help you. Websites like Coursera, Edx, and Codecademy offer Python courses that can help you learn the basics of the language.\n",
      "\n",
      "## Can I learn Python in a year?\n",
      "\n",
      "If you spend about 5 to 6 hours learning Python 3 every day, you’ll likely become proficient in it within a year and may even become fully competent in about six months. However, this is an intensive schedule and may not be suitable for everyone.\n",
      "\n",
      "## What is the hardest programming language to learn?\n",
      "\n",
      "1. C++\n",
      "2. Prolog.\n",
      "3. Haskell.\n",
      "4. Rust.\n",
      "5. Go.\n",
      "6. Ada.\n",
      "7. Lisp.\n",
      "8. F#\n",
      "\n",
      "## Can I teach myself Python?\n",
      "\n",
      "If you want to learn Python on your own, there are plenty of resources available to help you. Websites like Coursera, Edx, and Codecademy offer Python courses that can help you learn the basics of the language.\n",
      "\n",
      "## How do I practice Python for interviews?\n",
      "\n",
      "1. LeetCode. LeetCode is one of the best platforms to practice Python for interviews.\n",
      "2. HackerRank.\n",
      "3. Codewars.\n",
      "4. Interview Cake.\n",
      "5. Cracking the Coding Interview.\n",
      "6. Practice Python projects.\n",
      "7. Mock Interviews.\n",
      "8. Python specific interview websites.\n",
      "\n",
      "## Which country is famous for Python?\n",
      "\n",
      "Python has a lot of users in India, and it is among the top programming languages used in India. According to a 2019 survey by Stack Overflow, Python is one of the most used programming languages in India.\n",
      "\n",
      "## Is Python enough for data science?\n",
      "\n",
      "Is Python enough for data science? Yes, Python is a popular and versatile language for data science. It’s a great tool for data analysis, visualization, and working with large data sets. Many of the most popular data science tools and libraries, like Pandas, NumPy, and Matplotlib, are written in Python.\n",
      "\n",
      "## Can I learn Python in 2 months?\n",
      "\n",
      "Python is a big language with tons of features, so it’s impossible to become an expert in just one week. However, you can definitely become proficient and start writing simple Python programs in a week. If you’re keen to learn the basics, and want to spend around 5 hours a week studying, you could realistically master the language in three months.\n",
      "\n",
      "## Is Python good for gaming?\n",
      "\n",
      "Python is an excellent choice for creating games. It’s easy to learn, has a wide range of libraries for game development\n"
     ]
    }
   ],
   "source": [
    "generate_answer(pipe=finetuned_pipe, tokenizer=finetuned_tokenizer, prompt=\"what project can I work on as a Python beginner?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|im_start|>assistant\\n'\n",
      " 'There are several photographers who specialize in shooting luxury homes and '\n",
      " 'have a style that resembles or aligns with the work of Jose Villa. Please '\n",
      " 'note that I cannot guarantee that they exactly replicate his style, but '\n",
      " \"their work might share certain elements or aesthetics similar to Villa's. \"\n",
      " 'Here are a few photographers you might consider:\\n'\n",
      " '\\n'\n",
      " '1. Paul Barbera (<https:\\\\/\\\\/www.paulbarbera.com>): Known for his '\n",
      " \"cinematic, lifestyle, and architectural photography, Barbera's work often \"\n",
      " 'showcases luxury homes and interiors.\\n'\n",
      " '2. Mandy F enlightens <https:\\\\/\\\\/mandyfenlightens.com\\\\/>: With a focus on '\n",
      " \"architectural and interior photography, Mandy F's images often have a soft, \"\n",
      " \"dreamy quality, similar to Villa's work.\\n\"\n",
      " \"3. Susan Teare <https:\\\\/\\\\/www.susanteare.com\\\\/>: Teare's photography has \"\n",
      " 'a warm, natural, and elegantly styled aesthetic often found in luxury home '\n",
      " 'imagery.\\n'\n",
      " \"4. ALIX BOLIVAR <https:\\\\/\\\\/www.alixbolivar.com\\\\/>: Bolivar's work \"\n",
      " 'showcases high-end residences with a cinematic, storytelling quality, which '\n",
      " \"is comparable to Villa's style.\\n\"\n",
      " \"5. RachelWhiting.com <http:\\\\/\\\\/www.rachelwhiting.com\\\\/>: Whiting's \"\n",
      " 'photography often captures the beauty and luxury of residential and '\n",
      " \"commercial spaces with a warm, inviting quality similar to Villa's style.\\n\"\n",
      " '\\n'\n",
      " 'These are just a few examples, and you may find other photographers whose '\n",
      " \"work appeals to you. It's always a good idea to review their entire \"\n",
      " 'portfolios, not just a few images, to get a sense of their overall style and '\n",
      " 'consistency.<|im_end|>\\n')\n"
     ]
    }
   ],
   "source": [
    "ans = {\"prompt\":\"<|im_start|>system\\nYou are Dolphin, a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\nwho are photographers who photograph expensive homes but have a look like jose villa?<|im_end|>\\n\",\"chosen\":\"<|im_start|>assistant\\nThere are several photographers who specialize in shooting luxury homes and have a style that resembles or aligns with the work of Jose Villa. Please note that I cannot guarantee that they exactly replicate his style, but their work might share certain elements or aesthetics similar to Villa's. Here are a few photographers you might consider:\\n\\n1. Paul Barbera (<https:\\/\\/www.paulbarbera.com>): Known for his cinematic, lifestyle, and architectural photography, Barbera's work often showcases luxury homes and interiors.\\n2. Mandy F enlightens <https:\\/\\/mandyfenlightens.com\\/>: With a focus on architectural and interior photography, Mandy F's images often have a soft, dreamy quality, similar to Villa's work.\\n3. Susan Teare <https:\\/\\/www.susanteare.com\\/>: Teare's photography has a warm, natural, and elegantly styled aesthetic often found in luxury home imagery.\\n4. ALIX BOLIVAR <https:\\/\\/www.alixbolivar.com\\/>: Bolivar's work showcases high-end residences with a cinematic, storytelling quality, which is comparable to Villa's style.\\n5. RachelWhiting.com <http:\\/\\/www.rachelwhiting.com\\/>: Whiting's photography often captures the beauty and luxury of residential and commercial spaces with a warm, inviting quality similar to Villa's style.\\n\\nThese are just a few examples, and you may find other photographers whose work appeals to you. It's always a good idea to review their entire portfolios, not just a few images, to get a sense of their overall style and consistency.<|im_end|>\\n\",\"rejected\":\"<|im_start|>assistant\\nHello! As a helpful and respectful assistant, I'm here to assist you with your question. Jos\\u00e9 Villa is a well-known photographer known for his luxurious and dreamy style, often capturing high-end properties and estates. If you're looking for photographers who have a similar look and style to Jos\\u00e9 Villa, here are a few recommendations:\\n\\n1. Jacob Sutton: Jacob is a photographer specializing in high-end architecture and interior photography. His work often features vibrant colors, dramatic lighting, and a sense of grandeur that is reminiscent of Jos\\u00e9 Villa's style.\\n2. Paul Barbera: Paul is a photographer with a background in fine art and a keen eye for detail. His photography often captures the elegance and sophistication of luxury properties, with a focus on natural light and textures.\\n3. Jesse Johnston: Jesse is a photographer who has worked with some of the world's top architectural and design firms. His photography often features modern and contemporary spaces, with a focus on clean lines, natural light, and simplicity.\\n4. Mike Kelley: Mike is a photographer and cinematographer who specializes in high-end real estate and architectural photography. His work often features sweeping vistas, dramatic lighting, and a sense of drama that is reminiscent of Jos\\u00e9 Villa's style.\\n5. Vincent Versace: Vincent is a photographer and filmmaker who has worked with some of the world's top brands and celebrities. His photography often features a mix of glamour, sophistication, and whimsy, making him a great choice for photographing high-end properties with a unique flair.\\n\\nI hope these recommendations are helpful in finding a photographer with a style similar to Jos\\u00e9 Villa! Remember, it's important to research and compare different photographers to find the best fit for your needs and budget.<|im_end|>\\n\"}\n",
    "\n",
    "pprint(ans[\"chosen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Prompt**:\n",
      "who are photographers who photograph expensive homes but have a look like jose villa?\n",
      "\n",
      "**Generated Answer**:\n",
      "https://imgur.com/a/j22j7\n",
      "\n",
      "https://imgur.com/a/tqk9B\n",
      "\n",
      "\n",
      "I know jose villa isn't the only one, there are many other photographers who have this style, but his is probably the most popular and well known one. I'm trying to find photographers who have that same style, but when I search for \"jose villa style photographer\" or \"jose villa inspired photographer\" I mostly get jose villa himself or other photographers who are clearly inspired by his work, but none of them seem to have his exact style or quality.\n",
      "\n",
      "Is there any way I can find photographers who have a similar style or quality to jose villa?\n",
      "User 2: You can start by going to his website and look at the photographers he's mentioned as influences or that he admires. Also check out the other photographers who are featured in the same magazines or have similar styles, such as Jonas Peterson, Tara Donne, and Antonis Achilleos.\n",
      "User 1: Thanks! I did find a lot of them by looking at Jose Villa's influences, but it's hard to find newer photographers who have that same style. Most of them seem to be from the past or don't have the same level of quality that jose villa has.\n",
      "User 2: Oh, that's a tough one. His style is quite distinct and I agree it's hard to find others who match it. One approach you could try is looking through wedding blogs and magazines for photographers who shoot weddings in a similar style. The blogs and magazines that feature his work might also feature other photographers with similar styles. Good luck!\n",
      "User 1: Thanks! I'll definitely give that a try. I guess there's a reason why his style is so popular and recognizable.\n"
     ]
    }
   ],
   "source": [
    "generate_answer(pipe=base_pipe, tokenizer=base_tokenizer, prompt=\"who are photographers who photograph expensive homes but have a look like jose villa?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Prompt**:\n",
      "who are photographers who photograph expensive homes but have a look like jose villa?\n",
      "\n",
      "**Generated Answer**:\n",
      "I've always wanted to be a photographer, but I'm also very interested in interior design and architecture, so I'm thinking about going into that field too. The way they're able to make a space look beautiful, inviting, and sophisticated all at once is something I've always admired.\n",
      "\n",
      "It's difficult to pinpoint exactly who photographers are that are similar to Jose Villa, as there are many talented photographers in the industry with their own unique styles. However, there are a few photographers who share a similar aesthetic and approach to photographing high-end homes:\n",
      "\n",
      "1. Eric Piasecki: Eric is a well-known photographer in the world of architecture and interior design. His work often showcases elegant, luxurious homes in a way that emphasizes both their beauty and the craftsmanship that went into creating them.\n",
      "\n",
      "2. Gray Malin: Gray is a photographer known for his vibrant, colorful photographs of interiors and exteriors of beautiful homes. His work often has a dreamy quality to it, which can make even the most ordinary spaces feel like they're part of a fairy tale.\n",
      "\n",
      "3. Tessa Neustadt: Tessa is a photographer who specializes in capturing the details of beautiful homes and spaces. Her photographs often highlight the unique features of each space, making them feel special and one-of-a-kind.\n",
      "\n",
      "4. Kara Rosenlund: Kara is a photographer who often shoots interior spaces with a natural, organic aesthetic. Her work often feels warm and inviting, as if you could step right into the spaces she captures.\n",
      "\n",
      "5. Ryan Garvin: Ryan is a photographer who captures the architectural details of homes and spaces in a way that emphasizes both their beauty and their functionality. His work often has a modern, minimalist aesthetic that showcases the design of each space.\n",
      "\n",
      "These photographers are just a few examples of those who share a similar aesthetic to Jose Villa when it comes to photographing expensive homes. If you're interested in pursuing a career in this field, it's important to develop your own unique style and approach to photography while still being inspired by the work of others.\n",
      "\n",
      "As for getting into the field of interior design and architecture, you may want to consider pursuing a degree or certification in one of these fields, as well as gaining experience through internships or entry-level positions. This can help you build a strong foundation in the industry and allow you to eventually start your own business or work for a larger firm.\n",
      "\n",
      "Remember that building a successful career in this field often requires a combination of talent, hard work, and networking. Don't be afraid to reach out to other professionals in the industry, attend industry events, and continually educate yourself about the latest trends and techniques in interior design and architecture.\n",
      "\n",
      "Overall, if you're passionate about capturing the beauty of expensive homes and have a keen eye for design, there's no reason why you can't succeed in this field. Just be sure to stay true to your own unique vision and style, and don't be afraid to learn from the work of others who share your passion.\n"
     ]
    }
   ],
   "source": [
    "generate_answer(pipe=finetuned_pipe,tokenizer=finetuned_tokenizer, prompt=\"who are photographers who photograph expensive homes but have a look like jose villa?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Prompt**:\n",
      "who are photographers who photograph expensive homes but have a look like jose villa?\n",
      "\n",
      "**Generated Answer**:\n",
      "I've been searching for a while now and I can't seem to find any photographers who have a similar style to jose villa. I'm looking for photographers who do high end residential photography, but their photos have a similar style to jose villa's work.\n",
      "\n",
      "Here is a link to his website: http://www.josevilla.com/\n",
      "\n",
      "I'm looking for photographers who have a similar style of lighting, composition, and overall aesthetic.\n",
      "\n",
      "Any help would be greatly appreciated!\n",
      "User 0: I don't know if he's exactly what you're looking for, but [Gregory Black](http://www.gregoryblack.com/) has a similar style and also does high-end residential photography.\n",
      "User 2: Thanks for the suggestion! His work is definitely similar to Jose Villa's. I'm going to keep looking, but this is a good start.\n",
      "User 0: You're welcome! There are a few other photographers who do similar work, but they're not all focused on residential photography. I'd recommend looking into wedding photographers who do a lot of high-end, romantic work, as they often have a similar aesthetic.\n",
      "\n",
      "[Jonas Peterson](http://www.jonaspeterson.com/) is another photographer who does a lot of high-end residential work, and his style is similar to Villa's.\n",
      "\n",
      "[Jeremy Chou](http://www.jeremychou.com/) is another one.\n",
      "\n",
      "[Jonas Maon](http://www.jonasmaon.com/) is another one.\n",
      "\n",
      "[Jeff Canavan](http://www.jeffcanavan.com/) is another one.\n",
      "\n",
      "[Matthew Williams](http://www.matthewwilliamsphotography.com/) is another one.\n",
      "\n",
      "There are a lot of photographers out there who do similar work, but they might not all be focused on residential photography. It's worth looking into their portfolios to see if their style matches what you're looking for.\n",
      "User 2: Thanks for the suggestions! I'll definitely check them out. I'm looking for photographers who focus on residential photography, but it seems like a lot of them also do weddings. I'm going to keep looking, but these are definitely good leads.\n",
      "User 0: You're welcome! I'm sure you'll find someone who matches your style. Good luck!мм интересный фотограф.\n",
      "User 2: Thanks! I'm going to keep looking, but I've found a few photographers who have a similar style to Jose Villa. I'm going to reach out to them and see if they're available for residential work.\n",
      "\n",
      "Here's a list of photographers I've found so far:\n",
      "\n",
      "1. [Jose Villa](http://www.josevilla.com/) - The photographer I originally mentioned.\n",
      "2. [Gregory Black](http://www.gregoryblack.com/) - His work is similar to Jose Villa's.\n",
      "3. [Jonas Peterson](http://www.jonaspeterson.com/) - Another photographer with a similar style.\n",
      "4. [Jeremy Chou](http://www.jeremychou.com/) - Similar style to Jose Villa.\n",
      "5. [Jonas Maon](http://www.jonasmaon.com/) - Similar style to Jose Villa.\n",
      "6. [Jeff Canavan](http://www.jeffcanavan.com/) - Similar style to Jose Villa.\n",
      "7. [Matthew Williams](http://www.matthewwilliamsphotography.com/) - Similar style to Jose Villa.\n",
      "\n",
      "I'm going to reach out to these photographers and see if they're available for residential work. Thanks for your help!\n",
      "User 0: You're welcome! I'm sure you'll find someone who matches your style and is available for residential work. Good luck!м интересный фотограф.\n",
      "User 2: Thanks! I've reached out to a few of them and I'm waiting to hear back. I'm hoping to find someone who can capture the same aesthetic as Jose Villa for our project.\n",
      "\n",
      "I'm also going to keep looking for more photographers with a similar style. If you come across any more, please let me know!\n",
      "User 0: Of course! I'll keep an eye out for any photographers who match your style. Good luck with your project!м интересный фотограф.\n",
      "User 2: Thanks! I really appreciate your help. I'll let you know if I find any more photographers with a similar style.\n",
      "\n",
      "In the meantime, I've also been looking at interior design magazines and websites to get inspiration for our project. There are a lot of great photographers out there who do high-end residential work, so it's been helpful to see what kind of styles are popular right now.\n",
      "\n",
      "Here are a few magazines and websites that I've been looking at:\n",
      "\n",
      "1. [Architectural Digest](http://www.architecturaldigest.com/) - A well-known magazine that features high-end residential design and photography.\n",
      "2. [Dwell](http://www.dwell.com/) - Another magazine that features modern residential design and photography.\n",
      "3. [Houzz](https://www.houzz.com/) - A website that features residential design and photography, as well as advice and tips for homeowners.\n",
      "4. [Lonny](http://www.lonny.com/) - A magazine and website that features residential design and photography, with a focus on style and aesthetics.\n",
      "\n",
      "I hope this helps! Let me know if you have any other suggestions or if you come across any more photographers with a similar style to Jose Villa.\n",
      "\n",
      "Thanks again!м интересный фотограф.\n",
      "User 0: You're welcome! I'm glad you're finding inspiration for your project. Those magazines and websites are great resources for finding photographers and designers with a similar style. Good luck with your project! Интересный фотограф.\n",
      "User 2: Thanks! I've been looking through the magazines and websites you suggested, as well as reaching out to photographers with a similar style. I'm hoping to find someone who can capture the same aesthetic as Jose Villa for our project.\n",
      "\n",
      "I'm also going to keep looking for more photographers with a similar style. If you come across any more, please let me know!\n",
      "\n",
      "Thanks again for your help! Интересный фотограф.\n",
      "User 0: Of course! I'll keep an eye out for any photographers who match your style. Good luck with your project! Интересный фотограф.\n",
      "User 2: Thanks! I've been looking through the magazines and websites you suggested, as well as reaching out to photographers with a similar style. I'm hoping to find someone who can capture the same aesthetic as Jose Villa for our project.\n",
      "\n",
      "I'm also going to keep looking for more photographers with a similar style. If you come across any more, please let me know!\n",
      "\n",
      "Thanks again for your help! Интересный фотограф.\n",
      "User 0: You're welcome! I'm glad you're finding inspiration for your project. Those magazines and websites are great resources for finding photographers and designers with a similar style. Good luck with your project! Интересный фотограф.\n",
      "User 2: Thanks! I've been looking through the magazines and websites you suggested, as well as reaching out to photographers with a similar style. I'm hoping to find someone who can capture the same aesthetic as Jose Villa for our project.\n",
      "\n",
      "I'm also going to keep looking for more photographers with a similar style. If you come across any more, please let me know!\n",
      "\n",
      "Thanks again for your help! Интересный фотограф.\n",
      "User 0: Of course! I'll keep an eye out for any photographers who match your style. Good luck with your project! Интересный фотограф.\n",
      "User 2: Thanks! I've been looking through the magazines and websites you suggested, as well as reaching out to photographers with a similar style. I'm hoping to find someone who can capture the same aesthetic as Jose Villa for our project.\n",
      "\n",
      "I'm also going to keep looking for more photographers with a similar style. If you come across any more, please let me know!\n",
      "\n",
      "Thanks again for your help! Интересный фотограф.\n",
      "User 0: You're welcome! I'm glad you're finding inspiration for your project. Those magazines and websites are great resources for finding photographers and designers with a similar style. Good luck with your project! Интересный фотограф.\n",
      "User 2: Thanks! I've been looking through the magazines and websites you suggested, as well as reaching out to photographers with a similar style. I'm hoping to find someone who can capture the same aesthetic as Jose Villa for our project.\n",
      "\n",
      "I'm also going to keep looking for more photographers with\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "generate_answer(pipe=finetuned_pipe,tokenizer=finetuned_tokenizer, prompt=\"who are photographers who photograph expensive homes but have a look like jose villa?\",\n",
    "                temperature=0.5, top_k=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate open LLMs on MT-Bench\n",
    "\n",
    "For our use case we will use [MT-Bench](https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/README.md). MT-Bench is a Benchmark designed by LMSYS to test the conversation and instruction-following capabilities of large language models (LLMs). It evaluates LLMs through multi-turn conversations, focusing on their ability to engage in coherent, informative, and engaging exchanges. Since human evaluation is very expensive and time consuming, LMSYS uses GPT-4-Turbo to grade the model responses. Their paper shows as 80% agreement between strong LLM and human preferences. The [LMSYS leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) is updated regularly (last updated February 2, 2024). MT-Bench is part of the [FastChat Repository](https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/README.md). \n",
    "\n",
    "MT-Bench supports two different evaluation stratgies: \n",
    "* single-answer grading: LLM grade and give a score to model's answer directly on a scale of 10\n",
    "* pair-wise comparison: Compare two models and see which one is better using LLM as judge, resulting in a win-rate.\n",
    "\n",
    "We are going to use the pair-wise comparison method to compare the base SFT Model with the DPO model, to see if aligning the model with DPO improved the model. Running pairwise comparison on MT-Bench includes the following steps:\n",
    "1. Clone the FastChat Repository & install the requirements\n",
    "2. Generate Responses using our SFT (original) & DPO (trained) model\n",
    "3. Evaluate the responses using pair-wise comparison and GPT-4-Turbo as Judge\n",
    "4. Plot and compare the results\n",
    "\n",
    "MT-Bench currenlty only support OpenAI or Anthropic as Judge, where GPT-4 is the best. If you don't have access to GPT-4 you need to use a different evaluation method. I forked the FastChat repository and added GPT-4 Turbo reference answers to keep the cost lower.\n",
    "\n",
    "Note: If you use this example to train different model, e.g. llama you need to make sure that your model is registered and support in FastChat. This means you need:\n",
    "* [a registered conversation template](https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/conversation.py#L1024-L1035)\n",
    "* [a moodel adapter](https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L1491-L1504) used to match the model path\n",
    "* [register the model adapter](https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L2255)\n",
    "\n",
    "The easiest way to do this is to fork my [repository](https://github.com/philschmid/FastChat) and then add your model. In our example the base model of is `cognitivecomputations/dolphin-2.1-mistral-7b`, which is [already registered in FastChat](https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L1579).\n",
    "\n",
    "### 1. Clone the FastChat Repository & install the requirements\n",
    "\n",
    "Let's start by cloning the FastChat repository and installing the requirements.\n",
    "\n",
    "_Note: Restart your notebook/kernel to clear up all GPU memory._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone main branch of FastChat\n",
    "!git clone https://github.com/philschmid/FastChat.git\n",
    "# Install FastChat with model worker and llm_judge dependencies\n",
    "!pip install -e \"./FastChat[model_worker,llm_judge]\"\n",
    "!pip install matplotlib tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate Responses using our SFT (original) & DPO (trained) model\n",
    "\n",
    "To Generate the responses in MT-Bench we need our directory into `FastChat/fastchat/llm_judge` and then run the `gen_model_answer.py` script. This will generate the responses and save them into a file. We will use the default `--max-new-token` length of `1024`, which could lead to some truncation. If you want to avoid truncation you can increase the `--max-new-token` length to `1512` or higher.\n",
    "\n",
    "We change into the `FastChat/fastchat/llm_judge` directory to run all the evaluation scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/katopz/book/src/ml/finetuning/FastChat/fastchat/llm_judge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katopz/book/src/ml/finetuning/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%cd {os.getcwd()}/FastChat/fastchat/llm_judge\n",
    "# should be in FastChat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with the SFT model and then the DPO model. \n",
    "\n",
    "_Note: The answer of the models will be stored to `FastChat/fastchat/llm_judge/data/mt_bench/model_answer`. You might want to save them later for additional evaluation, when you have a new fine-tuned model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to data/mt_bench/model_answer/mistral-dolphin-sft.jsonl\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# make sure that is the correct path\n",
    "model_path=\"cognitivecomputations/dolphin-2.1-mistral-7b\"\n",
    "# model id will be used to load our conversation template https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L1579\n",
    "model_id=\"mistral-dolphin-sft\"\n",
    "\n",
    "# generate model answer\n",
    "!python gen_model_answer.py --model-id {model_id} --model-path {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Generating all responses can take a while, ~60 minutes or more._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate the responses using the DPO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to data/mt_bench/model_answer/mistral-dolphin-dpo.jsonl\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Change this to where you saved the model during training, remember our current directory is FastChat/\n",
    "model_path=\"../../../doplhin-dpo\"\n",
    "\n",
    "# model id will be used to load our conversation template https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/model/model_adapter.py#L1579\n",
    "model_id=\"mistral-dolphin-dpo\"\n",
    "\n",
    "# generate model answer\n",
    "!python gen_model_answer.py --model-id {model_id} --model-path {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Generating all responses can take a while, ~120 minutes or more._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate the responses using pair-wise comparison and GPT-4-Turbo as Judge\n",
    "\n",
    "After we have the responses we can evaluate them using the `gen_judgment.py` script. This will pairwise compare all the responses using GPT-4-Turbo and rate which response is better.\n",
    "\n",
    "_Note: We need an `OPENAI_API_KEY` with access to GPT-4 Turbo, running MT-Bench will cost ~1-2$ per model evaluation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai_key=\"sk-xxxx\" # replace with your openai key\n",
    "\n",
    "# Pairwise comparison of the two models using OpenAI's GPT-4 Turbo\n",
    "!OPENAI_API_KEY={open_ai_key} python gen_judgment.py --model-list \"mistral-dolphin-dpo\" \"mistral-dolphin-sft\" --judge-model \"gpt-4-1106-preview\" --mode \"pairwise-all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: This can take ~70 minutes._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot and compare the results\n",
    "\n",
    "After we have the results we can plot them and compare the win-rate of the SFT and DPO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are saved at the following location, make sure its correct\n",
    "res = \"./data/mt_bench/model_judgment/gpt-4-1106-preview_pair.jsonl\"\n",
    "\n",
    "!python show_result.py --input-file {res} --model-list \"mistral-dolphin-dpo\" \"mistral-dolphin-sft\" --judge-model \"gpt-4-1106-preview\" --mode pairwise-all\n",
    "\n",
    "# display plot from image file\n",
    "from PIL import Image\n",
    "i = Image.open(\"win_rate_gpt-4-1106-preview.png\")\n",
    "i.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| model               |   win |   loss |   tie |   win_rate |   loss_rate |   win_rate_adjusted |\n",
    "|:--------------------|------:|-------:|------:|-----------:|------------:|--------------------:|\n",
    "| mistral-dolphin-dpo |    45 |     17 |    98 |    0.28125 |     0.10625 |              0.5875 |\n",
    "| mistral-dolphin-sft |    17 |     45 |    98 |    0.10625 |     0.28125 |              0.4125 |\n",
    "\n",
    "![image](../assets//win_rate_gpt-4-1106-preview.png)\n",
    "\n",
    "By using DPO we were able to achieve a win-rate of 0.5875 compared to 0.4125 with the SFT model. This means by applying DPO we tuned our model to generate responses, which are more aligned with what humans/AI would prefer. This is not optimal yet, but it's a good start. \n",
    "\n",
    "Since the guide is only a starting point, you should consider additional evaluation methods, e.g. human evaluation or instruction-following capabilities. This means we might not have reached the full potential of the model. You should consider training for more epochs and on a larger dataset to improve the model further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clean up the FastChat Repository\n",
    "\n",
    "Since we temporary cloned the FastChat repository we can now clean it up by deleting the directory.\n",
    "\n",
    "_Note: If you want to keep your evaluation results you should save the `model_answer` and `judgment` directory._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../../..\n",
    "\n",
    "# delete the cloned repository\n",
    "!rm -rf FastChat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
