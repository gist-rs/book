# Vision-Language Models (VLMs)

- [LLaVA-NeXT (LLaVA-1.6)](https://github.com/haotian-liu/LLaVA)
- [MoE-LLaVA](https://github.com/PKU-YuanGroup/MoE-LLaVA): Mixture of Experts for Large Vision-Language Models [ü§ó](https://huggingface.co/spaces/LanguageBind/MoE-LLaVA)
- [DeepSeek-VL-7B-Chat](https://arxiv.org/pdf/2403.05525.pdf): [ü§ó](https://huggingface.co/deepseek-ai/deepseek-vl-7b-chat)
- [Qwen-VL-Chat-1.1](https://github.com/QwenLM/Qwen-VL?tab=readme-ov-file): [ü§ó](https://huggingface.co/Qwen/Qwen-VL-Chat), [üóÑÔ∏è](https://github.com/QwenLM/Qwen-VL)
- [TextMonkey](https://github.com/Yuliang-Liu/Monkey): An OCR-Free Large Multimodal Model for Understanding Document
- [DocOwl 1.5](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5): Unified Structure Learning for OCR-free Document Understanding, [sum](https://modelscope.cn/studios/iic/mPLUG-DocOwl/summary)
- [UReader](https://github.com/LukeForeverYoung/UReader): Universal OCR-free Visually-situated Language Understanding
- [MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation?tab=readme-ov-file): A Comprehensive Evaluation Benchmark for Multimodal LLM
- [VILA-2.7b](https://huggingface.co/Efficient-Large-Model/VILA-7b): Vision Model from NVIDIA and MIT, [demo](https://vila-demo.hanlab.ai/), [ü§ó](https://huggingface.co/collections/Efficient-Large-Model/vila-on-pre-training-for-visual-language-models-65d8022a3a52cd9bcd62698e)
